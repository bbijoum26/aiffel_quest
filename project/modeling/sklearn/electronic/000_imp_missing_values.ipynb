{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023 전력 사용량 예측 AI 경진대회\n",
    "\n",
    "- 대회 안내\n",
    "    - 배경: 안정적이고 효율적인 에너지 공급을 위해서는 전력 사용량에 대한 정확한 예측 필요\n",
    "           따라서, 한국에너지공단에서는 전력 사용량 예측 시뮬레이션을 통한 효율적인 인공지능 알고리즘 발굴이 목표\n",
    "    - 주제: 전력 사용량 예측 AI 모델 개발\n",
    "    - 설명: 건물 정보와 시공간 정보를 활용하여 특정 시점의 전력 사용량을 예측하는 AI 모델 개발\n",
    "\n",
    "- Data Info.\n",
    "    - train.csv\n",
    "        - train 데이터: 100개이 건물들의 2022/06/01 ~ 2022/08/24 까지의 데이터\n",
    "        - 일시별 기온, 강수량, 풍속, 습도, 일조, 일사 정보 포함\n",
    "        - 전력 사용량(kWh)포함\n",
    "    - building _info.csv\n",
    "        - 100개 건물 정보\n",
    "        - 건물 번호, 건물 유형, 연면적, 냉방 면적, 태양광 용량, ESS 저장 용량, PCS 용량\n",
    "            - ESS(Energy Storage System): 에너지 저장 시스템은 전력을 저장하고 나중에 필요할 때 이를 공급하거나 소비하는 시스템\n",
    "                - 주로 전력 그리드 안정화, 재생 에너지 효율 향상, 비상 대응 등의 목적으로 사용된다.\n",
    "                - ESS는 에너지를 축적하여 사용자의 요구에 따라 전력의 공급과 소비를 조절할 수 있으며, 이로써 전력 시스템의 안정성과 효율성을 향상시킬 수 있다.\n",
    "                - ESS 기술: 배터리, 슈퍼커패시터, 소스 연료전지\n",
    "            - PCS(Power Conersion System): 전력 변환 시스템은 다양한 에너지 소스와 부하 사이의 에너지 변환과 전송을 담당하는 장치\n",
    "                - 에너지 소스로부터 생성된 전력을 용도에 맞게 변환하여 전력 그리드에 공급하거나, 전력 그리드에서 가져온 전력을 부하로 전달하는 연할을 수행\n",
    "                - PCS의 전력의 품질 유지, 효율적인 에너지 변환, 스마트 그리드 구축 등에 중요한 역할을 함\n",
    "                - PCS의 구성요소: 인버터, 컨버터, 제어 시스템 등\n",
    "    - test.csv\n",
    "        - test 데이터: 100개의 건물들의 2020/08/25 ~ 2022/08/31 까지의 데이터\n",
    "        - 일시별 기온, 강수량, 풍속, 습도의 예보 정보\n",
    "\n",
    "- 필요한 지식\n",
    "    - 계절 및 기상정보: 계절적 변동이나 기상 조건이 전력 사용량에 어떤 영향을 미치는 분석\n",
    "    - 건물 정보: 건물의 특성이나 장비에 따라 전력 사용량 차이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전력 사용량 분석 자료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주거용 건물의 전력 사용량에 대한 시계열 분석 및 예측\n",
    "출처: 박경미 and 김재희.(2019). 주거용 건물의 전력 사용량에 대한 시계열 분석 및 예측. 응용통계연구, 32(3), 405-421\n",
    "<br/>\n",
    "- 초록\n",
    "    - 얼마나 많은 에너지를 사용하느냐에 대한 예측은 사회에서 중요한 이슈이다. 특히 주거 건물은 건물의 특성상 다른 건물에 비해 예측하기 힘들다.\n",
    "    - 본 논문에서는 주거용 건물의 전력 사용량에 대한 시계열 분석의 방뻐들을 설명하고자 한다.\n",
    "        - 일반적으로 온도는 전력 사용량과 밀접한 관련이 있다고 알려져 있다. 변수들 사이에 공적분 관계가 존재한다면, 시간에 따른 오차를 조정하는 방법인 오차수정모형을 적용한다.\n",
    "        - 전력 사용량과 온도를 포함한 변수들의 사이에 공적분 관계가 있음 보이고, 새로운 온도 반응 함수를 정의하여 온도 효과를 고려한 오차수정모형을 적용하고자 한다.\n",
    "<br/><br/>\n",
    "- 사용 방법\n",
    "    - 시간 단위로 나눈 전력 수요 자료로 외부의 영량이 가장 적은 오전 5시에서 오전 6시까지의 전략 사용량 추정(Holt-Winters, TBATS 비교)\n",
    "    - 시간 단위로 나눈 전력 수요 자료의 함수적 군집 분석\n",
    "    - 시간 단위로 나눈 전력 사용량의 시계열 패턴을 군집 및 분류 분석\n",
    "<br/><br/>\n",
    "- 서론\n",
    "    - 공적분과 오차수정모형은 1987년 Engle, Granger에 의해 처음 제시되었다.\n",
    "        - 각 나라에서 전력 수요량을 비롯한 에너지와 경제 변수간의 관계를 파악하기 위하여 공적분과 오차수정모형을 이용한 연구들이 많이 진행되었다.\n",
    "            - 1997년, 공적분과 오차수정모형을 이용하여 1961년 ~ 1990년까지 한국과 GDP와 에너지 소비량의 관계를 보여줌\n",
    "            - 2003년, 멕시코의 주거용, 상업용, 공업용의 월별 전력 사용량과 소득과 가격, 온도를 변수로 사용하여 오차수정모령과 함께 시간변동계수와 온도반응함수 방법 사용\n",
    "            - 2006년, 비선형 모형은 인경신경망 방법을, 선형 모형은 다중 로그 선형 회귀, 반응 표면 회귀, 자기회귀누적 이동평균모형을 이용한 회귀분석을 제안하여 1990/01 ~ \n",
    "              2002/12 까지의 타이완의 전력 사용량 예측함\n",
    "<br/><br/>\n",
    "- 시계열\n",
    "    - 시계열 분석에서는 정상성을 만족하는 자료를 주로 사용한다. \n",
    "    - 정상성을 만족하기 위해서는 자료의 평균이 일정하고 분산이 시점 t에 의존하지 않으며, 공분산은 시차에만 의존해야한다.\n",
    "    - 이를 만족하지 않은 경우는 비정상 시계열이라고 한다.\n",
    "<br/>\n",
    "    - 정상 시계열\n",
    "        - 단위근 검정: 귀무가설이 기각되면 시계열 데이터가 정상성을 만족함을 의미한다.\n",
    "            - 즉, 정상성을 만족하면 단위근을 갖지 않는다.\n",
    "            - $ H_{0} : \\pi = 0 (단위근을 갖는다.) \\quad vs. \\quad H_{1} : \\pi \\neq 0 (단위근을 갖지 않는다.)$\n",
    "        - ARIMA: 자기회귀과정과 이동평균과정을 혼합한 형태인 ARMA\n",
    "        - ARMAX: ARMA 모형에 선형관계를 갖는 공변량을 추가\n",
    "        - GARCH: 시차를 가지는 조건부 이분산성을 뜻하는 자기회귀 조건부 이분산성을 일반화한 모형\n",
    "<br/>\n",
    "    - 비정상 시계열\n",
    "        - 공적분: 2개의 I(d) 변수의 선형결합이 d차 적분보다 낮게 나타나는 것\n",
    "            - 만약, 2개의 변수가 I(1)이라 가정할 때, 그것들의 선형 결합이 I(0)일 경우를 공적분 되었다고 한다.\n",
    "        - 그랑져 인과 검점: 변수들이 서로 관계가 있음을 알지만 어떤 변수가 원인 변수이고, 어떤 변수가 결과 변수인가 불분명할 경우 사용되는 검정\n",
    "            - 인과 관계를 보고자 하는 두 변수의 예측에 필요한 정보가 이들 변수들의 시계열에만 의존한다는 가정 하에 이루어진다.\n",
    "        - ECM\n",
    "            - 1단계: I(1) 변수들의 회귀식 추정 (I(1): 변수들의 장기 균형에서 온 오차)\n",
    "                - 오차가 정상성을 만족하지에 대한 여부는 DF 검정 또는 ADF 검정 이용\n",
    "            - 2단계: 단위근을 가지고 있다는 귀무가설을 기각하면, ECM 추정\n",
    "        - 모형 적합도 평가: 평균제곱오차, 평균제곱근오차, 오차율 사용\n",
    "<br/><br/>\n",
    "- 주거용 건물의 특성\n",
    "    - 전력 사용량은 계절별로 뚜렷한 특징을 보인다.\n",
    "        - 여름과 겨울에 에어컨이나 온열 기구 등의 사용으로 여름과 겨울에는 전력 사용량이 증가한다.\n",
    "        - 특히, 주거용 건물의 특성은 공업용, 상업용 건물과는 달리 아침과 아침과 저녁시간대에, 주중보다는 주말 및 공유일에 전력 사용이 많다.\n",
    "        - 따라서, 일반적인 경우에는 일정한 주기로 주말마다 전력 사용량이 많아지기 때문에 6, 7 사이의 주기를 고려할 수 있다.\n",
    "<br/><br/>\n",
    "- 온도반응함수\n",
    "    - 대부분 온도 변수를 사용하기 위해서 평균 기온이나 최저 기온 또는 최고 기온을 사용한다.\n",
    "        - 여름은 온도가 높을 수록 냉방기구의 사용이 많아지고, 겨울은 온도가 낮을수록 난방기구의 사용이 많아지기 때문에 통상적으로 온도는 전력 사용에 영향을 끼친다는 것을 알 수 있다.\n",
    "        - 하지만 평균 기온이나 최저 기온 또는 최고 기온은 전력 사용 예측하는데 선형 효과를 줄 수 없기 때문에 온도의 효과를 전력 사용량에 반영하기 힘들다.\n",
    "    - 이를 보완하기 위해 전력 사요량에 반영될 수 있는 온도 변수의 선형 효과를 주기 위해 온도 변수를 온도반응함수로 변환해 준다.\n",
    "        - $g(s^{*}) = \\alpha + \\beta _{1} s^{*} + \\beta _{2} s^{*2} + \\beta _{3} cos \\pi s^{*} + \\beta _{4} cos \\pi s^{*}$\n",
    "        - $s^{*} = \\frac{s + 20}{60}$\n",
    "            - $g(s^{*})$ : 온도반응함수, $s^{*}$: 평균 온도를 0 ~ 1 사이 값으로 표준화한 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost 기법을 이용한 단기 전력 수요 예측 및 하이퍼파라미터 변화에 따른 영향 분석\n",
    "출처: 오재영, 함도현, 이용건 and 김기백(2019).XGBoost 기법을 이용한 단기 전력 수요 예측 및 하이퍼파라미터 변화에 따른 영향 분석. 전기학회논문지, 68(9), 1073-1078\n",
    "<br/><br/>\n",
    "3page까지 봄(유료 ㅠ)\n",
    "\n",
    "- 서론\n",
    "    - 전력 수요 예측은 전력 공급의 안정화, 효율적 운영, 발전기 시스템 운행 등에 큰 영향을 줄 수 있으므로 정확도가 떨어지는 전력 수요 예측은 계획되지 않은 설비의 사용으로 계통 안정도를 낮추고 추가적인 전력 구매 등의 경제적 손실을 발생시키게 된다.\n",
    "    - SVM, LSTM, DNN 등을 이용한 연구가 진행되었으며, 국내 전력 계통에서의 전력 수요예측 연구도 꾸준히 진행되고 있다.\n",
    "    - 전력 계통 운용을 계획하기 위해서는 시간별 전력 수요 예측에서부터 연간 전력 수요 예측까지의 다양한 예측 기간에 대한 연구가 필요하다.\n",
    "    - 본 논문에서는 XGBoost를 이용한 전국 일주일 단기 전력 수요 예측 방법을 제안한다.\n",
    "        - 예측 모델 성능은 MAPE(mean abslute precentage error)를 기준으로 평가\n",
    "<br/><br/>\n",
    "- XGBoost를 이용한 전력 수요 예측 과정\n",
    "    - 시간 및 요일을 입력 특징(feature)으로 고려하다.\n",
    "        - 시간대별 및 요일별은 범주형(categorical) 데이토로서, 시간은 0 ~ 23시를 1 ~ 24까지, 요일은 월 ~ 일까끼를 0 ~ 6까지의 정수로 변환\n",
    "    - XGBoost 툭징 중요도 기능을 사용하여 입력 특징을 선별한다.\n",
    "        - XGBoost를 이용해 학습을 진행한 후, 특징 중요도를 사용하면 모델 학습에 높은 중요도를 가진 입력 특징을 수치적으로 확인할 수 있다. (weight, gain, coverage)\n",
    "        - weight: 트리구조 학습과정에서 트리 분기의 기준되는 특징의 발생빈도를 기준으로 한다.\n",
    "        - gain: 노드를 분기할 때, 엔트로피의 감소량을 기준으로 중요도를 산정\n",
    "        - coverage: 리프노드의 분기에 결정된 입력 특징의 발생빈도를 기준으로 중요도 산출\n",
    "<br/><br/>\n",
    "- 하이퍼파라미터 최적화 방법\n",
    "    - 격자 탐색(grid search): 하이퍼파라미터 공간을 격자로 분할한 뒤, 모든 격자 지점에 대한 모델 성능을 확인해보는 방법\n",
    "        - 격자 형태로 하이퍼파라미터를 찾을 경우, 비교적 균등하고 전체적인 탐색이 가능하다\n",
    "        - 간격이 너무 촘촘하거나, 확인해야 할 하이퍼파라미터의 개수가 늘어나면 탐색의 시간이 기하급수적으로 증가한다.\n",
    "    - 랜덤 탐색(random search): 하이퍼파라미터 공간에서 특정 구간의 하이퍼파라미터 값을 랜덤하게 설정하여 학습을 시도해 본 후, 그 중 가장 성능이 뛰어난 하리퍼파라미터 값을 싸용\n",
    "        - 하이퍼파라미터의 모든 값에 대해서 학습하면, 그만큼 학습의 횟수도 많아지기 때문에 효율적인 방법이 될 수 없다.\n",
    "        - 랜덤하게 탐색을 하여 학습횟수는 줄이면서 높은 성능을 보장할 하이퍼파라미터를 찾을 가능성이 있다. (단, 랜덤한 경우에 따른 학습이므로, 최적이라는 보장 X)\n",
    "    - 베이지안 최적화(bayesian optimization): 통계학적인 방법을 이용해 하이퍼파라미터를 최적화하는 방법\n",
    "        - 탐색된 몇 지점에서의 성능이 확인되며 가우시안 분포와 같은 방식응 사용하여, 나머지 성능의 분포를 확률적으로 추정한다.\n",
    "            - 학습할 하이퍼파라미터 공간을 계산하여, 효율적인 하이퍼파라미터 탐색이 가능하도록 한다.\n",
    "        - 통계적인 방법을 사용하므로 랜덤 탐색의 경우와 마찬가지로 최적의 값을 찾는다는 보장하기는 힘들다.\n",
    "<br/><br/>\n",
    "- 하이퍼파라미터 최적화 과정\n",
    "    - 일반 파라미터(general parameter), 학습 파라미터(lerning task parameter), 부스트 파라미터(boost parameter)\n",
    "        - 일반 파라미터와 학습 파라미터는 다른 머신러닝 모델과 크게 다르지 않으므로, 부스트 파라미터에 초점을 맞추어 하이퍼파라미터 분석\n",
    "    - 부스트 파라미터: 트리 모델의 크기를 결정하는 값, 데이터의 샘플링을 결정하는 값, 정규화를 결정하는 값 등 세가지 종류로 나눌 수 있다.\n",
    "        - max_depth, min_child_weight, subsample, gamma를 본 연구에서 분석\n",
    "            - max_depth: 하나의 트리 모델의 형성과정에서 최대 트리 깊이 지정\n",
    "            - min_child_weight: 노드 분기를 결정할 때, 한 노드에 있는 데이터 인스턴스 가중치 합의 최댓값으로 설명\n",
    "            - gamma: 트리 모델의 복잡도를 제한해주는 값\n",
    "            - subsample: 트리마다 사용하는 데이터 샘플링 비율을 설정하는 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전력 사용량 분석\n",
    "1. 시계열 분석\n",
    "    - 시간에 따른 전력 사용량 정보를 포함하므로, 시계열 분석을 통해 계절성, 추세, 주기 등을 확인할 수 있다.\n",
    "    - 주어진 기간 동안의 전력 사용량 추리를 그래프로 시각화하고, 계절적인 변동 패턴이나 특이점을 찾을 수 있다.\n",
    "<br/><br/>\n",
    "2. 기상 데이터와의 관계\n",
    "    - 기온, 강수량, 풍속, 습도, 일조, 일사와 전력 사용량 간의 관계를 분석할 수 있다.\n",
    "    - 상관관계 분석이나 산점도 그래프를 통해 기상 조건이 전력 사용량에 어떤 영향을 미치는지 확인할 수 있다.\n",
    "<br/><br/>\n",
    "3. 건물 특성과의 관계 분석\n",
    "    - 건물 정보 중에서 건물 유형, 연면적, 냉방 면적, 태양관 용량 등을 활용하여 전력 사용량과의 관계를 분석할 수 있다.\n",
    "<br/><br/>\n",
    "    - 건물 유형에 따른 전력 사용량 차이, 건물의 특성이 전력 사용량 예측에 어떤 영향을 미치는 등의 분석한다.\n",
    "<br/><br/>\n",
    "4. 통계적 분석\n",
    "    - 평균, 중앙값, 표준편차 등의 통계치를 계산하여 데이터의 대략적인 분포를 확인한다.\n",
    "    - 각 변수 간의 상관관계나 분포를 비교하여 유의미한 관계를 확인할 수 있다.\n",
    "<br/><br/>\n",
    "5. 시각화\n",
    "    - 막대 그래프, 선 그래프, 히스토그램, 상자 그림 등 다양한 그래프를 사용하여 데이터를 시각화\n",
    "    - 시각화를 통해 데이터의 패턴이나 관계를 직관적으로 파악할 수 있다.\n",
    "<br/><br/>\n",
    "6. 머신러닝 모델\n",
    "    - 데이터셋을 활용하여 전력 사용량 예측 모델 구축\n",
    "    - 회귀 모델, 시계열 모델, 머신러닝 알고리즘 등을 사용하여 전력 사용량을 예측하고 모델의 성능을 평가한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 회귀 모델 종류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 선형 회귀(Linear Regression)\n",
    "    - 가장 기본적이고 널리 사용되는 회귀분석 알고리즘\n",
    "    - 입력변수(X)와 출력변수(y)간의 선형 관계를 모델링하는데 사용된다.\n",
    "    - 주어진 입력변수에 대해 출력변수의 값을 예측하는 것이 목표이다.\n",
    "    - 선형 회귀 모델 가정\n",
    "        - 선형관계: 입력변수와 출력변수 간의 관계가 선형이다고 가정<br/>\n",
    "                   즉, 출력변수가 입력변수에 대해 일정한 비율로 변화한다고 가정\n",
    "        - 잔차의 정규성: 잔차(실제 출력과 모델의 예측값 간의 차이)가 정규 분포를 따른다고 가정\n",
    "    - 선형회귀는 주어진 데이터를 기반으로 회귀 계수를 추정하고, 이를 사용하여 새로운 입력 변수에 대한 출력값을 예측한다.\n",
    "        - 회귀계수: 일반적으로 최소 제곱법을 사용하여 추정된다.\n",
    "        - 최소 제곱법: 잔차의 제곱의 합을 최소화하는 회귀 계수를 선택하는 방식\n",
    "<br/><br/>\n",
    "```python\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # 모델 생성\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # 모델 학습\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 모델 예측\n",
    "    y_pred = model.predixt(X_test)\n",
    "\n",
    "    # 모델 평가\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 릿지 회귀(Ridge Regression)\n",
    "    - 선형 회귀의 한 종류로, L2(유클리드 거리) 규제를 추가하여 모델을 안정화하는 기법\n",
    "    - 선형 회귀와 마찬가지로 입력변수와 출력변수 간의 선형 관계를 모델링하며, 과적합을 감소시키기 위해 회긔  계수에 패널티를 부여한다.\n",
    "    - $minimize J(\\beta) = MSE(\\beta) + \\alpha \\sum_{i=1}^{n} \\beta_{i}^{2}$\n",
    "        - $J(\\beta)$: 목적 함수\n",
    "        - $MSE(\\beta)$: 평균 제곱 오차로, 실제 출력과 모델의 예측 간의 제곱의 평균을 나타낸다.\n",
    "        - $\\beta$: 회귀 계수(가중치)\n",
    "        - $\\alpha$: 규제 강도를 조절하는 매개변수(릿지 회귀의 중요한 하이퍼파라미터)\n",
    "    - 릿지 회귀 특징\n",
    "        - 과적합 방지: L2 규제를 통해 과적합을 줄일 수 있다. <br\\>\n",
    "                     모델의 복잡도를 줄여서 일반화 성능을 향상시킨다.\n",
    "        - 다중 공선성 해결: 릿지 회귀는 다중 공선선 문제를 해결하는데 도움이 된다.\n",
    "            - 다중 공선서이 압력 변수 간의 높은 상관관계로 인해 회귀 계수의 추정이 불안정해지는 문제\n",
    "        - 회귀 계수 축소: 릿지 회귀는 회귀 계수를 축소시켜 중요하지 않은 변수의 영향을 줄인다.\n",
    "            - 변수 선택의 효과를 가질 수 있다.\n",
    "<br/><br/>\n",
    "\n",
    "```python\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # 모델 생성\n",
    "    alpha = 1.0 # 규제 강도 조절\n",
    "    model = Ridge(alpha=alpha)\n",
    "\n",
    "    # 모델 학습\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 모델 예측\n",
    "    y_pred = model.predixt(X_test)\n",
    "\n",
    "    # 모델 평가\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 라쏘 회귀(Lasso Regression)\n",
    "    - 선형 회귀의 한 종류로, L1(맨하탄 거리) 규제를 추가하여 모델을 안정화하는 기법\n",
    "    - L1 규제는 회귀 계수의 절댓값을 패널티로 적용하므로, 일부 회귀 계수를 정확히 0으로 만들어 변수 선택의 역할 을 할 수 있다.\n",
    "    - $minimize J(\\beta) = MSE(\\beta) + \\alpha \\sum_{i=1}^{n} \\left | \\beta_{i} \\right |$\n",
    "        - $J(\\beta)$: 목적 함수\n",
    "        - $MSE(\\beta)$: 평균 제곱 오차로, 실제 출력과 모델의 예측 간의 제곱의 평균을 나타낸다.\n",
    "        - $\\beta$: 회귀 계수(가중치)\n",
    "        - $\\alpha$: 규제 강도를 조절하는 매개변수(라쏘 회귀의 중요한 하이퍼파라미터)\n",
    "    - 라쏘 회귀 특징\n",
    "        - 변수 선택: L1 규제로 인해 일부 회귀 계수가 정확하게 0이 될 수 있다.<br/>\n",
    "                   따라서, 불필요한 변수를 제거하고 모델의 복잡도를 줄이는 효과를 가질 수 있다.\n",
    "        - 다중 공선성 해결: 라쏘 회귀도 다중 공선성 문제를 해결하는데 도움이 된다.\n",
    "        - 희소성 제공: 변수 선택을 통해 희소한(대부분 회귀 계수가 0인) 모델을 얻을 수 있다.\n",
    "<br/><br/>\n",
    "\n",
    "```python\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # 모델 생성\n",
    "    alpha = 1.0 # 규제 강도 조절\n",
    "    model = Lasso(alpha=alpha)\n",
    "\n",
    "    # 모델 학습\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 모델 예측\n",
    "    y_pred = model.predixt(X_test)\n",
    "\n",
    "    # 모델 평가\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 라쏘-릿지 회귀 (Elastic Net Regression)\n",
    "    - L1과 L2 규제를 혼합하여 선형 회귀를 개선한 모델로, 라쏘와 릿지의 장점을 결합\n",
    "    - $minimize J(\\beta) = MSE(\\beta) + \\alpha (\\frac{1-l1ratio}{2} \\sum_{i=1}^{n} \\beta_{i}^{2} + l1ratio \\sum_{i=1}^{n} \\left | \\beta_{i} \\right |)$\n",
    "        - $J(\\beta)$: 목적 함수\n",
    "        - $MSE(\\beta)$: 평균 제곱 오차로, 실제 출력과 모델의 예측 간의 제곱의 평균을 나타낸다.\n",
    "        - $\\beta$: 회귀 계수(가중치)\n",
    "        - $\\alpha$: 규제 강도를 조절하는 매개변수(라쏘 회귀의 중요한 하이퍼파라미터)\n",
    "        - $l1_ratio$: L1 규제의 비율을 나타내난 매개변수로, 0 ~ 1 사이의 값으로 조정\n",
    "            - $l1_ratio = 0$일 때는 릿지 회귀와 동일하며, $l1_ratio = 1$일 때는 라쏘 회귀와 동일하다.\n",
    "    - Elastic Net 회귀 특징\n",
    "        - 변수 선택: L1 규제로 인해 일부 회귀 계수가 정확하게 0이 될 수 있어 변수 선택 효과를 얻을 수 있다.\n",
    "        - 다중 공선성 해결: 다중 공선성 문제를 해결하는데 도움을 줄 수 있다.\n",
    "        - L1과 L2 규제의 효과 결합: L1과 L2 규제를 조합하여 두 규제의 장점을 활용할 수 있다.\n",
    "<br/><br/>\n",
    "```python\n",
    "    from sklearn.linear_model import ElasticNet\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # 모델 생성\n",
    "    alpha = 1.0     # 규제 강도 조절\n",
    "    l1_ratio = 0.4  # L1 규제 비율\n",
    "    model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "\n",
    "    # 모델 학습\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 모델 예측\n",
    "    y_pred = model.predixt(X_test)\n",
    "\n",
    "    # 모델 평가\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 다항 회귀 (Polynomial Regression)\n",
    "    - 선형 회귀의 확장된 형태로, 비선형 데이터를 모델링하는데 사용되는 기법\n",
    "    - 다항 회귀는 다항식 함수를 사용하여 입력변수와 출력변수 간의 관계를 모델링한다.\n",
    "    - $y = \\beta_{0} + \\beta_{1} x$\n",
    "        - 다항 회귀는 $x$의 고차항을 추가하여 모델을 확장한다.\n",
    "        - 예를 들어, 2차 다항 회귀는 다음과 같은 모델링할 수 있다.\n",
    "        - $y = \\beta_{0} + \\beta_{1} + \\beta_{2} x^{2}$\n",
    "<br/><br/>\n",
    "```python\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # 데이터 생성\n",
    "    X = np.random.rand(100, 1) * 10  # 입력 변수\n",
    "    y = 0.5 * X**2 + 2 * X + 1 + np.random.randn(100, 1) * 2  # 출력 변수 (2차 다항식 관계)\n",
    "\n",
    "    # 다항식 변환\n",
    "    poly = PolynomialFeatures(degree=2)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "\n",
    "    # 데이터 분할\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # 선형 회귀 모델 생성\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # 모델 학습\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 모델 평가\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    # 결과 시각화\n",
    "    plt.scatter(X, y, color='blue', label='Actual')\n",
    "    plt.scatter(X_test[:, 1], y_pred, color='red', label='Predicted')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 의사결정트리 회귀 (Decision Tree Regression)\n",
    "    - 데이터를 분류하거나 회귀하는데 사용되는 지도학습 알고리즘\n",
    "    - 데이터의 특징들을 기반으로 하여 의사 결정 규칙을 구성하고, 이를 통해 데이터를 예측하거나 분류하는 모델을 만든다.\n",
    "        - 의사결정트리는 계층적으로 질문을 해 나가면서 데이터를 분할하고, 각 분할 영역에서의 예측값을 결정한다.\n",
    "        - 이러한 과정에서 트리의 각 노드는 특정 특징이나 속성에 대한 결절 규칙을 갖고 있으며, 가장 하위 노드인 리프 노드에서 최적적으로 예측 결과를 얻는다.\n",
    "    - 의사결정트리 특징\n",
    "        - 해석 가능성: 트리의 각 노드는 직관적으로 이해할 수 있는 결정 규칙을 나타내며, 모델의 해석 가능성이 높다.\n",
    "        - 비선형 데이터 모델링: 의사결정트리는 비선형 관계를 모델링 할 수 있다.\n",
    "            - 데이터의 분포에 따라 유연하게 분할이 이루어진다.\n",
    "        - 자동적인 변수 선택: 트리 구성 과정에서 중요한 변수들이 상위 레벨에 위치하여 변수 선택과 변수 중요도를 자동으로 판단할 수 있다.\n",
    "        - 과대적합 문제: 트리가 깊어질수록 과대적합 문제가 발생할 수 있다.\n",
    "            - 이를 방지하지 위해 트리의 깊이를 제한하거나 가지치기 등의 방법을 사용한다.\n",
    "        - 앙상블 기법과 결합: 의사결정트리는 앙상 기법인 랜덤포레스트와 그래디언트 부스틍과 결합하여 더 강력한 모델을 구축할 수 있다.\n",
    " <br/><br/>\n",
    " \n",
    "```python\n",
    "    from sklearn.linear_model import DecisionTreeRegressor\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # 모델 생성\n",
    "    max_depth = 3\n",
    "    model = DecisionTreeRegressor(max_depth=max_depth)\n",
    "\n",
    "    # 모델 학습\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 모델 예측\n",
    "    y_pred = model.predixt(X_test)\n",
    "\n",
    "    # 모델 평가\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 랜덤 포레스트 회귀 (Random Forest Regression)\n",
    "    - 의사결정트리를 기반으로 한 앙상블 학습 알고리즘 중 하나로, 여러 개의 의사결정트리를 조합하여 더 강력하고 안정적인 모델을 구축하는 방법\n",
    "    - 랜덤 포레스트는 과대적합 문제를 줄이고 예측 성능을 향상시킬 수 있는 강력항 기법 중 하나\n",
    "    - 랜덤 포레스트 특징\n",
    "        - 부트스트랩 샘플링: 랜덤 포레스트는 입력 데이터에서 부트스트랩 샘플을 생성하여 각 의사결트리의 학습 데이터를 생성한다.\n",
    "            - 이를 통해 다양한 데이터 세트로 다양한 의사결정를 학습하게 된다.\n",
    "        - 랜덤한 특성 선택: 각 의사결정트리의 노드 분할 시에 랜덤하게 일부 특성들만을 고려하여 분할을 수행한다.\n",
    "            - 이를 통해 특성 조합을 활용하여 다양한 결정 규칙을 생성한다.\n",
    "        - 앙상블: 각 의사결정트리의 예측을 집계하여 최종 예측 결과를 도출한다.\n",
    "            - 분류 문제의 경우 각 트리의 클래스 예측을 다수결 방식으로 선택하고, 회귀 문제의 경우 각 트리의 예측값을 평균 또는 가중평균하여 최종 예측을 수행한다.\n",
    "     <br/><br/>\n",
    "\n",
    "     \n",
    "```python\n",
    "    from sklearn.ensemble import RandomForestREfressor\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # 모델 생성\n",
    "    n_estimators = 100\n",
    "    model = RandomForestREfressor(n_estimators=n_estimators)\n",
    "\n",
    "    # 모델 학습\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 모델 예측\n",
    "    y_pred = model.predixt(X_test)\n",
    "\n",
    "    # 모델 평가\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 서포트 벡터 머신 회귀 (Support Vector Machine Regression)\n",
    "    - 분류와 회귀 문제에 사용되는 지도학습 알고리즘 중 하나로, 데이터를 분리하는 최적의 초평면을 찾는 평면\n",
    "    - 서포트 벡터 머신은 데이터를 고차원 공간으로 변환하여 클래스를 구분하는 초평면을 찾는데, 이 때 서포트 벡터라 불리는 일부 데이터픞 포인트들의 초평면에 가장 가까이 위치하게 된다.\n",
    "    - SVM의 특징\n",
    "        - 최적 분리 초평면 찾기: SVM은 클래스 간의 간격을 최대화하는 초평면을 찾는 것을 목표로 한다.\n",
    "            - 이를 위해 데이터를 고차원 공간으로 매핑하고, 그 중에서 최적의 초평면을 찾아 데이터를 분류한다.\n",
    "        - 서포트 벡터: SVM에서는 결정 경게에 가장 까이 위치한 데이터 포인트들\n",
    "            - 서포트 벡터들은 결정 경계를 결정하는데 중요한 역할을 한다.\n",
    "        - 커널 트릭: SVM은 데이터를 고차원으로 변환할 때 커널 함수를 사용하여 실제로 고차원 공간으로 매핑하지 않고도 데이터를 분리 초평면으로 변환할 수 있다\n",
    "<br/><br/>\n",
    "```python\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # 모델 생성\n",
    "    model = SVR(kernel='linear')\n",
    "\n",
    "    # 모델 학습\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 모델 예측\n",
    "    y_pred = model.predixt(X_test)\n",
    "\n",
    "    # 모델 평가\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. 최근접 이웃 회귀 (K-Nearest Neighbors Regression)\n",
    "    - 지도학습 알고리즘 중 하나로, 주어진 데이터 포인트에 대해 가장 가까운 이웃들의 클래스를 기반으로 분류하거나 회귀를 수행하는 방법\n",
    "    - KNN은 간단하면서도 강력한 알고리즘으로, 주변 데이터의 패턴을 이용하여 새로운 데이터를 예측하는데 사용된다.\n",
    "    - KNN의 특징\n",
    "        - 이웃 선택: 주어진 데이터 포인트에 대해 가장 가까운 이웃들을 선택한다.\n",
    "            - 이웃의 개수는 K는 사용자가 지정한다.\n",
    "        - 거리 측정: 이웃들을 선택할 때, 거리 측정 방법을 사용하여 주어진 데이터 포인트와 다른 데이터 포인트 간의 거리르 계산한다.\n",
    "            - 일반적으로 유클리디안 거리나 맨하탄 거리를 사용한다.\n",
    "        - 다수결 투표(분류) 또는 평균(회귀)\n",
    "            - 분류 문제의 경우, 선택된 이웃들의 클래스를 다수결 투표로 결정하여 주저인 데이터의 클래스를 예측\n",
    "            - 회귀 문제의 경우, 선택된 이웃들의 출력의 평균을 계산하여 예측값을 얻음\n",
    "<br/><br/>\n",
    "```python\n",
    "    # 회귀\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    k = 5   # 이웃의 개수 설정\n",
    "    model = KNeighborsRegressor(n_neighbors=k)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predixt(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    # 분류\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    k = 5   # 이웃의 개수 설정\n",
    "    model = KNeighborsRegressor(n_neighbors=k)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predixt(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. 신경망 회귀 (Neural Network Regression)\n",
    "    - 신경망을 활용하여 회귀 문제를 해결하는 기법\n",
    "    - 신경망은 다층 퍼셉트론을 기반으로 하며, 복잡한 비선형 관계를 모델링할 수 있는 강력한 도구\n",
    "    - 신경망 회귀 특징\n",
    "        - 입력측과 은닉층: 입력 데이터를 받아들이는 입력층과 중간 계산을 수행하는 은닉층이 있다.\n",
    "            - 은닉층은 여러 개의 뉴런으로 구성되며, 각 뉴런은 활성화 함수를 통해 입력을 변환한다.\n",
    "        - 가중치와 편향: 신경망은 각 연결에 가중치와 편향을 가지고 있다.\n",
    "            - 이 가중치와 편향이 학습을 통해 조정되어 데이터의 패턴을 학습한다.\n",
    "        - 활성화 함수: 은닉층과 출력층의 뉴런은 활성화 함수를 통해 비선형 변환을 수행한다.\n",
    "            - 일반적으로 ReLU(Rectified Linear Activation), 시그모이드(sigmoid) 사용\n",
    "        - 손실 함수: 회귀 문제에서는 평균 제곱 오차와 같은 손실 함수를 사용하여 예측값과 실제값의 차이를 최소화하도록 학습한다.\n",
    "<br/><br/>\n",
    "```python\n",
    "    import tensorflow as tf\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # 신경망 모델 구성\n",
    "    model = tf.keras.Sequential([tf.keras.layers.Dense(64, activation='relu', input_sahpe=(입력 변수 개수, )), \n",
    "                                 tf.keras.layers.Dense(32, activation='relu'), \n",
    "                                 tf.keras.layers.Dense(1)])\n",
    "    # 모델 컴파일\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    # 모델 학습\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "    # 모델 예측\n",
    "    y_pred = model.predixt(X_test)\n",
    "\n",
    "    # 모델 평가\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. 회귀 신경망 (Regression Neural Network)\n",
    "    - 입력 데이터와 해당 입력에 대한 연속적인 출력 값을 예측하는데 사용되는 신경망 모델\n",
    "    - 회귀 문제에서 신경망은 주어진 입력 데이터로부터 연속적인 값을 예측하는 방법을 학습하게 된다.\n",
    "    - 회귀 신경망은 다층 퍼셉트론 아키텍처를 기반\n",
    "    - 회귀 신경망 구성요소\n",
    "        - 입력층: 입력 데이터를 받아들이는 역할\n",
    "            - 입력층의 노드 수는 입력 변수의 개수에 따라 결정된다.\n",
    "        - 은닉층: 하나 이상의 은닉층은 입력 데이터의 비선형 특성을 모델링하기 위해 사용된다\n",
    "            - 은닉층은 활성화 함수를 통해 입력을 변환하고, 여러 개의 뉴런으로 구성될 수 있다.\n",
    "        - 출력층: 회귀 문제에서는 하나의 출력 뉴런을 가지며, 예측한 연속적인 값을 출력한ㄴ다.\n",
    "        - 활성화 함수: 은닉층과 출력층의 뉴런에서 활성화 함수를 사용하여 비선형 변환을 수행한다.\n",
    "            - 일반적으로 ReLU, 시그모이드, 하이퍼볼릭 탄젠트 등의 활성화 함수가 사용된다.\n",
    "        - 손실 함수: 회귀 문제에서는 손실 함수로 평균 제곱 오차(MSE), 평균 절대 오차(MAE)등이 사용된다.\n",
    "        - 최적화 알고리즘: 역전파 알고리즘과 최적화 알고리즘(예: 확률적 경사 하강법)을 사용하여 가중치와 편향을 조정하면서 손실을 최소화한다.\n",
    "<br/><br/>\n",
    "```python\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # 데이터 로드 및 전처리\n",
    "    X = data[['입력 변수1', '입력 변수2', ...]].values\n",
    "    y = data['전력소비량'].values\n",
    "\n",
    "    # 데이터 정규화\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 데이터 분할\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # 신경망 모델 구성\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)  # 출력 뉴런 1개 (회귀 문제)\n",
    "    ])\n",
    "\n",
    "    # 모델 컴파일\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # 모델 학습\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "    # 모델 예측\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # 평가 및 결과 출력\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(\"Mean Squared Error:\", mse)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. XGBoost 회귀 (XGBoost Regression)\n",
    "    - eXtreme Gradient Boosting의 약자로, 앙상블 학습 알고리즘 중 하나\n",
    "    - XGBoost는 경사 부스팅(Gradient Boosting)을 기반으로 하며, 다양한 데이터 분류 및 회귀 문제에서 높은 예측 성능을 보이는 강력한 기법 중 하나로 널리 사용된다.\n",
    "    - XGBoost의 주요 특징\n",
    "        - Gradient Boosting 알고리즘: XGBoost는 경사 부스팅 알고리즘의 변형이다.\n",
    "            - 경사 부스팅은 약한 모델(예: 결정트리)을 순차적으로 학습시켜 이전 모델의 오차를 보완하는 방식으로 예측 모델을 구성한다.\n",
    "        - 규제와 복잡도 제어: XGBoost는 규제를 통해 과적합을 방지하고 모델의 복잡도를 제어할 수 있다.\n",
    "            - L1(Lasso), L2(Ridge) 규제를 지원한다.\n",
    "        - 트리 알고리즘: XGBoost는 결정 트리를 기반으로 하며, 최적의 분한을 찾아내어 예측 성능을 향상시킨다.\n",
    "            - 이진 분할(binary spliting)과 규제된 트리 복잡도를 활용한ㄴ다.\n",
    "        - 병렬 처리 및 고속성: XGBoost는 다양한 병렬 처리 기법을 사용하여 학습을 가속화하며, 대용량 데이터에도 높은 속도로 작동한ㄴ다.\n",
    "        - 결측치 처리: 결측치를 처리하는 기능을 내장하고 있어, 결측치가 있는 데이터에서도 안정적으로 작동한다.\n",
    "    - XGBoost 작동 방식\n",
    "        - 초기화: 첫 번째 모델을 학습하여 초기 예측을 만든다.\n",
    "        - 오차계산: 초기 예측과 실제 값 간의 오차를 계산한다.\n",
    "        - 트리 추가: 새로운 결정 트리를 학습하여 이전 모델의 오차를 보정한다.\n",
    "            - 새로운 모델은 이전 모델에서 잘못 예측한 샘플에 더 많은 가중치를 부여하며, 새로운 트리를 추가함으로써 오차를 줄인다.\n",
    "        - 반복: 위의 과정을 지정한 횟수(나무 개수)까지 반복 한다.\n",
    "            - 각 모델은 이전 모델들의 오차를 보정하여 예측 성능을 개선한다.\n",
    "<br/><br/>\n",
    "```python\n",
    "    import xgbosst as xgb\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # 모델 생성\n",
    "    model = xgb.XGBRegressor()\n",
    "\n",
    "    # 모델 학습\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 모델 예측\n",
    "    y_pred = model.predixt(X_test)\n",
    "\n",
    "    # 모델 평가\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- XGBoost는 Gradient Boosting Algorithm을 알아야 이해하기 쉽다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient Boosting을 알기 전에 의사결정나무, AdaBoost에 대해 먼저 알아보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "https://zephyrus1111.tistory.com/124"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 의사결정나무(Decision Tree)란?\n",
    "    - 입력값에 대한 예측값을 나무 형태로 나타내어주는 모형\n",
    "    - 용어\n",
    "        - 뿌리 마디(root node): 시작되는 마디로 전체 자료를 포함\n",
    "        - 자식 마디(childe node): 하나의 마디로부터 분리된 2개 이상의 마디들\n",
    "        - 부모 마디(parent node): 주어진 마디의 상위 마디\n",
    "        - 끝 마디(teminal node): 자식 마디가 없는 마디\n",
    "        - 중간 마디(internal node): 부모 마디와 지식 마디가 모드 있는 마디\n",
    "        - 가지(branch): 연결되어있는 2개 이상의 마디 집합\n",
    "        - 깊이(depth): 뿌리 마디로부터 끝마디까지 연결된 마디의 개수\n",
    "    - 장단점\n",
    "         - 장점\n",
    "            - 의사결정나무는 모형의 해석이 쉽다.\n",
    "            - 모형의 해석이 쉽다는 것은 입력값이 주어졌을 때 설명변수의 영역을 따라가며 출력값이 어떻게 나올 수 있는지 알기 쉽다\n",
    "        - 단점\n",
    "            - 예측력이 다른 지도 학습 모형에 비하여 떨어진다.\n",
    "            - 어쩌면 당연하다. 왜냐 의사결정나무는 설명변수의 일부 영역에서 단순히 평균 또는 다수결 법칙에 의하여 예측을 수행하기 때문이다.\n",
    "            - 특히 회귀나무의 예측력은 더 좋지 않은 편인데 이는 해당 마디에 포함된 반응 변수의 평균을 예측값으로 추정하는데 아무래도 평균을 사용하다보니 이상치에 영향을 받을 수 있기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 의사결정나무 모형 만들기\n",
    "    - 성장(growing): 성장단계에서는 먼저 최적화할 목접함수를 정한다.\n",
    "        - 각 마디에서 이 목적함수를 최적화하는 변수와 그 변수의 분리 기준을 찾아 의사결정나무를 성장시키며 사전에 정의된 정지규칙을 만족하며 성장을 중단한다.\n",
    "    - 가지치기(Pruning): 가지치기 단계에서는 과적합을 방지, 해석이 안 되는 규칙 등 불필요한 가지를 제거한다.\n",
    "    <br/><br/>\n",
    "    - 회귀나무(Regression Tree)\n",
    "        - $f(x) = \\sum_{m=1}^{M} c_{m} I (x \\in R_{m})$\n",
    "            - $R_{1}, ..., R_{m} 은 M개로 설명변수들의 (겹치지 않는) 영역$\n",
    "            - $설명변수 벡터 x가 R_{m}에 포함된다면 c_{m}으로 예측하겠다는 것$\n",
    "            - $c_{m}과 R_{m}에 의해 결정된다는 것을 알 수 있다.$\n",
    "        - 불순도(Impurity)\n",
    "            - $c_{m}, R_{m}$ 은 불순도라는 측도를 이용하여 정하게 된다.\n",
    "            - 회귀나무에서는 불순도의 측도로 오차 제곱합을 사용한다.\n",
    "        - 가치치기(Pruning)\n",
    "            - 앞선 내용들은 의사결정나무를 성장시키는 방법이다. 그렇다면 언제까지 성장시켜야 하는가?\n",
    "            - 너무 깊이가 깊은 나무는 과적합의 문제가 있고 깊이가 얕은 나무는 과소 적합의 문제가 있을 수 있기 때문이다.\n",
    "            - 이러한 문제를 해결하기 위해 나무 깊이가 너무 깊어지지 않도록 가지치기 하게 된다.\n",
    "                - 사전 가지치기: 사전에 정지 규칙을 만들어 나무의 성장 중지 <br/>\n",
    "                               사용할 깊이와 샘플 수는 검즈 데이터를 통하여 결정하는 것이 일반적이다.\n",
    "                - 사후 가지치기: 주어진 나무 $T_{0}$에 대하여 아래의 목적함수를 최소화 하는 나무 모형 $T_{\\alpha} \\subset T_{0}$를 찾는다. <br/> $C_{\\alpha}(T) = \\sum_{m=1}^{\\left | T \\right |} N_{m} Q_{m} (T) + \\alpha \\left | T \\right |$\n",
    "                <br/> \n",
    "                $\\left | T \\right |$: 끝마디 개수, $N_{m}$: 영역 $R_{m}$ 에서의 데이터 개수, $\\hat{c}_{m}$: $R_{m}에서의 $y_{i}$의 평균\n",
    "                <br/>\n",
    "                $Q_{m}(T) = \\frac{1}{N_{m}} \\sum_{x_{i} \\in R_{m}} (y_{i} - \\hat{c}_{m})^{2}$\n",
    "                    - $\\alpha \\geq 0$은 나무 모형의 복잡도와 적합도를 조절하는 Tuning Parameter로  $\\alpha$가 크면(작으면)  $T_{\\alpha}$의 크기는 작아(커)진다. <br/>\n",
    "                     $\\alpha = 0$이면 사후 가지치기를 하지 않고 $T_{0}$를 최종 의사결정나무로 선정한다. <br/>\n",
    "                      $\\alpha$는 교차검증법을 이용하여 추정한다.\n",
    "    <br/><br/>\n",
    "    - 분류나무(Classification Tree)\n",
    "        - 분류나무에서 분리 기준을 정하는 목적함수는 카이제곱 통계량, 지니 계수, 엔트로피 지수를 불순도의 측도로 사용하며 앞서 살펴본 회귀나무를 성장시키는 방법과 동일하다.\n",
    "            - $x \\in R_{m}$가 주어졌을 때 분류 나무의 예측값은 $R_{m}$에 속하는 $y_{i}$의 범주가 가장 많은 범주를 예측값으로 한다.\n",
    "            - 즉, 분류나무 예측값을 $R_{m}$에서 다수결의 원칙으로 정하는 것이다.\n",
    "        - 카이제곱 통계랑\n",
    "            - $X^{2} = \\sum_{k=1}^{4}(E_{k} - O_{k})^{2} / E_{k}$ <br/>\n",
    "              $E_{k}, O_{k}$: 각각 기대 도수와 관측도수 <br/>\n",
    "              분리 기준은 카이제곱 통계량이 최대가 되는 값으로 정한다.\n",
    "        - 지니 계수(Gini Index)\n",
    "            - $G = G_{L}P(Left) + G_{R}P(Right)$\n",
    "            - $G_{L} = 2 \\dot [P(Good in Left) P(Bad in Left)]$ <br/>\n",
    "                     = $P(Good in Left)[1 - P(Good in Left)] + P(Bad in Left)[1- P(Bad in LEft)]$\n",
    "            - $G_{R}$도 비슷하게 정의할 수 있다.\n",
    "            - 분리 기준은 지니 지수가 최소가 되도록 하는 값으로 정한다.\n",
    "        - 엔트로피 지수(Entropy Index)\n",
    "            - $E = E_{L}P(Left) + E_{R}P(Right)$\n",
    "            - $E_{L} = -P(Good in Left) log_{2} P(Bad in Left) - P(Bad in Left) log_{2} P(Bad in Left)$\n",
    "            - $E_{R}$도 비슷하게 정의된다.\n",
    "            - 분리 기준은 엔트로피 지수를 최소화시키는 값으로 정한다.\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "\n",
    "https://zephyrus1111.tistory.com/195"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. AdaBoost(Adaptive Boost)이란?\n",
    "    - AdaBoost(Adaptive Boost)의 준말로 초기 모형을 약한 모형으로 설정하며 매 스텝마다 가중치를 이용하여 이전 모형의 약점을 보환하는 새로운 모형을 순차적으로 적합한 뒤 최종적으로 이들을 선형 결합하여 얻어지 모형을 생성시키는 알고리즘\n",
    "    - 부스팅은 약한 모형(Week Learner)을 이용하여 강한 모형(Strong Learner)을 만들어내는 것이 특징인 알고리즘\n",
    "    - AdaBoost는 초기 모형을 아주 약한 모형으로 설정한 뒤 매 스텝마다 이전 모형의 약점을 보완하는 새로운 모형을 순차적으로 적합시킨다.\n",
    "        - 모형의 약점은 예측을 제대로 하지 못한 데이터가 존재함을 의미한다.\n",
    "        - AdaBoost 알고리즘은 이러한 비정상적 데이터에 빠르게 적응하여 예측력이 강한 모델을 실시간으로 학습한다고 하여 Adaptive라는 단어가 붙게 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Adaboost(Adaptive Boost) 알고리즘\n",
    "    - AdaBoost 분류기\n",
    "        - AdaBoosting 분류기를 만들어내는 알고리즘은 기존 이진 분류 문제를 더 일반화한 다중 분류에 대한 알고리즘을 소개한다.\n",
    "        - 알고리즘\n",
    "            - 가중치를 이용하는 방법은 weighted Gini Index 같은 것을 사용할 수 있는데 나는 가중치를 이용한 리샘플링 방법을 택했다\n",
    "        - 블로그 참조\n",
    "    - AdaBoost 회귀 모형\n",
    "        - $(x_{1}, y_{1}), ..., (x_{n}, y_{n})$ <br/>\n",
    "          $x_{i}$ 는 $P$ 차원(실수) 벡터이며 $y_{i}$는 실수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. AdaBoosting(Adaptive Boosting) 장단점\n",
    "    - 장점\n",
    "        - AdaBoost은 과적합의 영향을 덜 받는다\n",
    "            - 결합하는 모형의 개수 $M$이 증가함에 따라 에러가 잘 증가하지 않는 장점이 있다. <br/> \n",
    "            따라서 AdaBoost은 과적합의 여향을 덜 받는다. 물론 $M$이 아주 크면 과적합이 발생하지만 상대적으로 늦게 발생한다는 것이다.\n",
    "        - 구현이 쉽다.\n",
    "            - 알고리즘 자체가 어렵지 않아 기본 학습기가 잘 구현되어 있다면 AdaBoost 알고리즘 구현은 쉬워진다.\n",
    "        - 유연하다.\n",
    "            - 손실 함수를 여러가지 사용할 수 있으며 기본 학습기에 제한이 없다. <br/>\n",
    "              즉, 기본 학습기를 의사결정나무 뿐만아니라 다른 학습기(로지스틱 회귀 모형, 선형 회귀 모형 등)도 사용할 수 있다.\n",
    "    - 단점\n",
    "        - AdaBoost 알고리즘은 이상치에 민간하다고 한다.\n",
    "        - 해석이 어렵다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "https://zephyrus1111.tistory.com/224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Gradient Boosting이란?\n",
    "    - Gradient Boosting는 Gradient(또는 잔차(residual))를 이용하여 이전 모형의 약점을 보완하는 새로운 모형을 순차적으로 적합한 뒤 이들을 선형 결합하여 얻어진 모형을 생성하는 지도 학습 알고리즘\n",
    "    - Gradient Boosting은 잔차를 이용하여 이전 모형의 약점을 보완하다.\n",
    "    - Gradient Boosting은 순차적으로 적합한 뒤 이들을 선형 결합한 모형을 생성한다.\n",
    "    - Gradient Boosting은 지도 학습 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Gradient Boosting 장단점\n",
    "    - 장점\n",
    "        - 구현이 쉽다\n",
    "            - 알고리즘 자체가 구현하기 어려운 것이 아니므로 기본 학습기가 잘 구현되어 있다면 Gradient Boosting 알고리즘 구현은 쉬워진다.\n",
    "        - 정확도가 좋다\n",
    "            - 잔차를 계속해서 줄여나가는 방식으로 학습하기 때문에 정확도가 좋다\n",
    "        - 굉장히 유연하다\n",
    "            - 여기서 유연하다는 것은 기본 학습기에 제한되지 않아 의사결정나무 이외에 다른 모형을 써도 된다는 것과 여러 가지 손실 함수를 저용할 수 있다\n",
    "    - 단점 \n",
    "        - 과적합이 발생할 가능성이 크다\n",
    "            - Gradient Boosting 자체가 잔차를 계속해서 줄여나가는 방식인데 이는 장점이 될 수 있는 반면 Nois가 발생하는 겨웅 과적합이 발생할 수 있다.\n",
    "        - 메모리 문제가 있다.\n",
    "            - 반복수 $M$만큼의 나무가 필요하므로 반복수가 커지면 나무도 많아져 메모리를 사용해야할 수 있다.\n",
    "        - 해석이 어렵다.\n",
    "            - Gradient Boosting에서는 반복수 $M$과 축소 모드 $l$을 미리 지정해줘야 한다.\n",
    "            - 이 값은 보통 교차 검증을 통하여 설정할 수 있다. <br/>\n",
    "              이때 두가지 값을 모두 교차 검증할 수 있지만 보통 $M$을 100 정도로 설정하고 $l$만 교차 검증을 통해 선택한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "https://zephyrus1111.tistory.com/232 <br/>\n",
    "https://en.wikipedia.org/wiki/XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. XGBoost란?\n",
    "    - XGBoost는 Gradient Tree Boosting 알고리즘에 과적합 방지를 위한 기법이 추가된 지도 학습 알고리즘\n",
    "        - XGBoost는 Gradient Tree Boosting이다.\n",
    "            - XGBoost는 기본 학습기를 의사결정나무로 하며 Gradient Boosting과 같이 Gradient(잔차)를 이용하여 이전 모형의 약점을 보완하는 방식으로 학습한다.\n",
    "        - XGBoost는 과적합 방지를 위한 기법이 추가된 알고리즘이다.\n",
    "            - XGBoost는 Gradient Tree Boosting에 과적합 방지를 위한 파라미터\n",
    "            $(\\lambda , \\gamma)$가 추가된 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. XGBoost 장단점\n",
    "    - 장점\n",
    "        - 과적합 방지가 잘되어 있다.\n",
    "            - 기존 Gradient Boosting의 약점인 과적합을 방지하기 위해 알고리즘 곳곳에 방지를 위한 장치를 마련해 두었다.\n",
    "        - 예측 성능이 좋다\n",
    "            - 과적합 방지가 잘되어 있어 이에 따라 예측 성능이 좋아졌다\n",
    "    - 단점\n",
    "        - 작은 데이터에 대하여 과적합 가능성이 있다.\n",
    "            - XGBoost가 충분히 좋은 예측 성능을 보이기 위해선 많은 데이터가 필요하다고 한다.\n",
    "        - 해석이 어렵다\n",
    "            - 이건 모든 앙상블 계열 알고리즘이 갖고 있는 근원적인 문제이며 XGBoost 또한 각 입력 변수에 대하여 출력 변수가 어떻게 변하는지에 대한 해석이 어렵다\n",
    "    - 고려사항\n",
    "        - XGBoost는 기본 학습기를 의사결정나무로 한다. 이때 의사결정나무의 깊이가 문제가 되는데 교차검증을 통해 계산해볼 수도 있겠지만 보통 6을 사용한다고 한다.\n",
    "        - 나머지 Hyper Paremeter인 $M, l, \\gamma, \\lambda$는 교차 검증을 통해 정해야 할 것이다."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
