{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76970cff",
   "metadata": {},
   "source": [
    "# í”„ë¡œì íŠ¸: KoChatGPT ì—…ê·¸ë ˆì´ë“œ í•˜ê¸°\n",
    "KoChatGPT ì†ŒìŠ¤ì½”ë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ì–‘í•œ ëª¨ë¸ ê°œì„  ì „ëµì„ ì„ íƒí•´ KoChatGPTë¥¼ ì—…ê·¸ë ˆì´ë“œí•´ ë³¸ë‹¤.  \n",
    "ì œì‹œëœ ì „ëµ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ê±°ë‚˜ ì—¬ëŸ¬ ê°œë¥¼ ì¡°í•©í•˜ì—¬ custom ChatGPTë¥¼ ê°œë°œ\n",
    "\n",
    "í‰ê°€ë¬¸í•­\n",
    "1. ê¸°ì¡´ KoGPT2ì™€ SFT ì ìš© ëª¨ë¸ ê²°ê³¼ ë¶„ì„í–ˆëŠ”ê°€?\n",
    "    - ê¸°ì¡´ ëª¨ë¸ì˜ ê²°ê³¼ë¬¼ê³¼ SFTë¥¼ ì ìš©í•œ ëª¨ë¸ì˜ ê²°ê³¼ë¬¼ì„ ì •ëŸ‰/ì •ì„±ì ìœ¼ë¡œ ë¹„êµ/ë¶„ì„í–ˆë‹¤.\n",
    "2. SFT ëª¨ë¸ê³¼ RM ëª¨ë¸ ê²°ê³¼ ë¶„ì„ì„ í•´ë³´ì•˜ëŠ”ê°€?\t\n",
    "    - SFTë¥¼ ì ìš©í•œ ëª¨ë¸ì˜ ê²°ê³¼ë¬¼ê³¼ RMì„ ì ìš©í•œ ëª¨ë¸ì˜ ê²°ê³¼ë¬¼ì„ ì •ëŸ‰/ì •ì„±ì ìœ¼ë¡œ ë¹„êµ/ë¶„ì„í–ˆë‹¤.\n",
    "3. ë°ì´í„°ì…‹ ì •ì œ / ìƒˆë¡œìš´ ë°ì´í„°ì…‹ / foundation model êµì²´ ì¤‘ í•˜ë‚˜ë¥¼ ì´ìš©í•´ ì •ëŸ‰ì  ì„±ëŠ¥ í–¥ìƒì„ í•´ë³´ì•˜ëŠ”ê°€?\t\n",
    "    - ê¸°ì¡´ ë°ì´í„°ì…‹ì„ ì¶”ê°€ë¡œ ì •ì œí•˜ê³ , generation ì„±ëŠ¥ì„ ì˜¬ë¦¬ê¸° ìœ„í•œ ê¸°ë²•(Beam search, Top-k sampling ë“±)ì„ ì‹¤í—˜í•´ ëª¨ë¸ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ë‹¤.\n",
    "    - ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•´ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ë‹¤.\n",
    "    - ë” ì ì ˆí•œ í•™ìŠµ ì „ëµ(SFT, RM, PPO)ì„ ì ìš©í•˜ê±°ë‚˜ initial modelì„ ë³€ê²½í•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f1b38b",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "```python\n",
    "!pip uninstall torch -y\n",
    "!pip install torch==1.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "\n",
    "# for transformers, ìµœì‹ ë²„ì „ì€ ì—ëŸ¬ë°œìƒ\n",
    "!pip install transformers==4.35.2\n",
    "!pip install accelerate==0.24.1\n",
    "\n",
    "# for ColossalAI\n",
    "!pip install colossalai==0.2.7\n",
    "\n",
    "# setup data\n",
    "!git clone https://github.com/airobotlab/KoChatGPT\n",
    "!mv KoChatGPT/data_kochatgpt .\n",
    "!mv KoChatGPT/img .\n",
    "\n",
    "%cd KoChatGPT/colossalai_ChatGPT_230319/\n",
    "!pip install .\n",
    "%cd ../../\n",
    "\n",
    "# setup library\n",
    "!pip install openai\n",
    "!pip install langchain==0.0.113\n",
    "!pip install pandas>=1.4.1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71283a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(\"Torch version:{}\".format(torch.__version__))\n",
    "print(\"Cuda version: {}\".format(torch.version.cuda))\n",
    "print(\"transformers version: {}\".format(transformers.__version__))\n",
    "print(\"cudnn version:{}\".format(torch.backends.cudnn.version()))\n",
    "print(\"GPU ì‚¬ìš© ê°€ëŠ¥ì—¬ë¶€: {}\".format(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568ef195",
   "metadata": {},
   "source": [
    "## ê¸°ì¡´ ë°ì´í„°ì…‹ ì¶”ê°€ ì •ì œ\n",
    "\n",
    "data_kochatgpt í´ë”  \n",
    "- kochatgpt_1_SFT.jsonl : SFTë¥¼ ìœ„í•œ promptì™€ completion ë¬¸ì¥ì…‹\n",
    "    - prompt: ëª¨ë¸ì´ ì‘ë‹µì„ ìƒì„±í•˜ê¸° ìœ„í•´ ë°›ëŠ” ì…ë ¥ ë¬¸ì¥\n",
    "    - completion: í•´ë‹¹ \"prompt\"ì— ëŒ€í•œ ì˜¬ë°”ë¥¸ ì‘ë‹µ ë˜ëŠ” ì™„ì„± ë¬¸ì¥\n",
    "- kochatgpt_1_RM.jsonl : RM í•™ìŠµì„ ìœ„í•œ promptì™€ ì„¸ ê°€ì§€ ranking ë¬¸ì¥ì…‹  \n",
    "- kochatgpt_1_PPO.jsonl : promt ë¬¸ì¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577e033b",
   "metadata": {},
   "source": [
    "### ë°ì´í„° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf7cee8",
   "metadata": {},
   "source": [
    "#### JSONL íŒŒì¼ ë¡œë”© í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbee41e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7c3d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_data = load_jsonl('./data_kochatgpt/kochatgpt_1_SFT.jsonl')\n",
    "rm_data = load_jsonl('./data_kochatgpt/kochatgpt_2_RM.jsonl')\n",
    "ppo_data = load_jsonl('./data_kochatgpt/kochatgpt_3_PPO.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964ce49e",
   "metadata": {},
   "source": [
    "### ë°ì´í„°ì…‹ EDA ë° ì „ì²˜ë¦¬\n",
    "ì£¼ì–´ì§„ ë°ì´í„°ì…‹(kochatgpt_1_SFT.jsonl, kochatgpt_1_RM.jsonl, kochatgpt_1_PPO.jsonl)ì— ëŒ€í•œ   \n",
    "íƒìƒ‰ì  ë°ì´í„° ë¶„ì„(EDA)ì„ ìˆ˜í–‰í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë°ì´í„° ì „ì²˜ë¦¬ ë° ì •ì œ ì‘ì—… ìˆ˜í–‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e7023f",
   "metadata": {},
   "source": [
    "#### ë°ì´í„°ì…‹ EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e74b57",
   "metadata": {},
   "source": [
    "**sft_data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777ffdf3",
   "metadata": {},
   "source": [
    "- ë°ì´í„° ì •ë³´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc542ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18755e3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sft_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad616fce",
   "metadata": {},
   "source": [
    "- ê°€ì¥ ë§ì´ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ íƒìƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1056268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter(\" \".join(sft_data['prompt']).split()).most_common(10)\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f558568",
   "metadata": {},
   "source": [
    "- 'prompt' ì—´ì—ì„œ ê°€ì¥ ë§ì´ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ íƒìƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fbc466",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_data['prompt_length'] = sft_data['prompt'].apply(len)\n",
    "sft_data['completion_length'] = sft_data['completion'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec0c609",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sft_data[['prompt_length', 'completion_length']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3857f060",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(sft_data['prompt_length'], bins=50, alpha=0.5, label='Prompt Length')\n",
    "plt.hist(sft_data['completion_length'], bins=50, alpha=0.5, label='Completion Length')\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.title('Length Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d86dfea",
   "metadata": {},
   "source": [
    "- 'completion' ì—´ì—ì„œ ë¬¸ì¥ ëì´ ì˜¨ì ('.')ìœ¼ë¡œ ëë‚˜ëŠ” ë¹„ìœ¨ ê³„ì‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e29525",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_with_period = sft_data['completion'].apply(lambda x: x.endswith('.')).mean()\n",
    "print(f\"Percentage of completions that end with a period: {end_with_period * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de802a2a",
   "metadata": {},
   "source": [
    "**rm_data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4552b954",
   "metadata": {},
   "source": [
    "- ë°ì´í„° ì •ë³´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd8065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2ea4a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rm_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53bfbf4",
   "metadata": {},
   "source": [
    "- ê°€ì¥ ë§ì´ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ íƒìƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acd3428",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter(\" \".join(rm_data['prompt']).split()).most_common(10)\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8ab90d",
   "metadata": {},
   "source": [
    "- 'prompt' ì—´ì—ì„œ ê°€ì¥ ë§ì´ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ íƒìƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ed2a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_data['prompt_length'] = rm_data['prompt'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e8b168",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rm_data['prompt_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9c55aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(rm_data['prompt_length'], bins=50, alpha=0.5, label='Prompt Length')\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.title('Length Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781dc790",
   "metadata": {},
   "source": [
    "**ppo_data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b3fcb6",
   "metadata": {},
   "source": [
    "- ë°ì´í„° ì •ë³´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7759d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fc8dd2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ppo_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc229bd",
   "metadata": {},
   "source": [
    "- ê°€ì¥ ë§ì´ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ íƒìƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758fa1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter(\" \".join(ppo_data['prompt']).split()).most_common(10)\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3206f9f8",
   "metadata": {},
   "source": [
    "- 'prompt' ì—´ì—ì„œ ê°€ì¥ ë§ì´ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ íƒìƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c174194",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_data['prompt_length'] = ppo_data['prompt'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04679cb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ppo_data['prompt_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e6162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(ppo_data['prompt_length'], bins=50, alpha=0.5, label='Prompt Length')\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.title('Length Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa81257",
   "metadata": {},
   "source": [
    "#### ë°ì´í„° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471af26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # HTML íƒœê·¸ ì œê±°\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # ì´ë©”ì¼ ì£¼ì†Œ ì œê±°\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n",
    "    # URL ì œê±°\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    # íŠ¹ìˆ˜ ë¬¸ì ë° ìˆ«ì ì œê±° (ì˜µì…˜)\n",
    "    text = re.sub(r'[^ê°€-í£\\s]', '', text)\n",
    "    return text.strip()  # ì–‘ìª½ ê³µë°± ì œê±°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7ccce4",
   "metadata": {},
   "source": [
    "- ë°ì´í„°ì— í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜ ì ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99f164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_data['prompt'] = sft_data['prompt'].apply(clean_text)\n",
    "sft_data['completion'] = sft_data['completion'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda5cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_data['prompt'] = rm_data['prompt'].apply(clean_text)\n",
    "rm_data['completion_0'] = rm_data['completion_0'].apply(clean_text)\n",
    "rm_data['completion_1'] = rm_data['completion_1'].apply(clean_text)\n",
    "rm_data['completion_2'] = rm_data['completion_2'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c44c4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_data['prompt'] = ppo_data['prompt'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dcdd1b",
   "metadata": {},
   "source": [
    "#### ë°ì´í„° jsonë¡œ ë³€ê²½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec679d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_data.to_json('./data_kochatgpt/data_cleaning_sft.jsonl', orient='records', lines=True, force_ascii=False)\n",
    "rm_data.to_json('./data_kochatgpt/data_cleaning_rm.jsonl', orient='records', lines=True, force_ascii=False)\n",
    "ppo_data.to_json('./data_kochatgpt/data_cleaning_ppo.jsonl', orient='records', lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3fb1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = [json.loads(line) for line in file]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef7a4f5",
   "metadata": {},
   "source": [
    "## SFT(Supervised Fine Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf87914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sft = load_jsonl('./data_kochatgpt/data_cleaning_sft.jsonl')\n",
    "data_sft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc93bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, pipeline\n",
    "from transformers import Trainer, TrainingArguments, AutoModelWithLMHead\n",
    "from copy import deepcopy\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoTokenizer, BloomTokenizerFast\n",
    "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import copy\n",
    "import logging\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n",
    "from typing import Optional, Dict, Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b4eeb0",
   "metadata": {},
   "source": [
    "### define argment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aefad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_path_1_SFT', type=str, default='./data_kochatgpt/data_cleaning_sft.jsonl')\n",
    "parser.add_argument('--model_name', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n",
    "parser.add_argument('--max_epochs', type=int, default=2)\n",
    "parser.add_argument('--train_batch_size', type=int, default=8)\n",
    "parser.add_argument('--output_dir', type=str, default='./output_cleaning_sft')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# for test\n",
    "args.model_name = 'skt/kogpt2-base-v2'  # SK GPT2, https://github.com/SKT-AI/KoGPT2\n",
    "# args.model_name = 'ajoublue-gpt2-base'  # ì•„ì£¼ëŒ€, https://github.com/HeegyuKim/language-model\n",
    "\n",
    "args.max_epochs = 2\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6f556a",
   "metadata": {},
   "source": [
    "### test & load skt gpt2 kroean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03c5099",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "                                                    bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "                                                    pad_token='<pad>', mask_token='<mask>')\n",
    "print(tokenizer.tokenize(\"ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f55e072",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "text = 'ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ”'\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "gen_ids = model.generate(input_ids,\n",
    "                         max_length=128,\n",
    "                         repetition_penalty=2.0,\n",
    "                         pad_token_id=tokenizer.pad_token_id,\n",
    "                         eos_token_id=tokenizer.eos_token_id,\n",
    "                         bos_token_id=tokenizer.bos_token_id,\n",
    "                         use_cache=True)\n",
    "generated = tokenizer.decode(gen_ids[0])\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08da52c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "generation_args = dict(\n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=375, # \\n\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "generator(\n",
    "    [\"0 : **ëŠ” ê²Œì„ ì¢‹ì•„í•˜ë‹ˆ\\n1 :\",\n",
    "    \"0 : ì–´ì œ ê°•ë‚¨ì—ì„œ ì‚´ì¸ì‚¬ê±´ ë‚¬ëŒ€ ã…œã…œ ë„ˆë¬´ ë¬´ì„œì›Œ\\n1 : í— ì™œ? ë¬´ìŠ¨ ì¼ ìˆì—ˆì–´?\\n0 : ì‚¬ì§„ë³´ë‹ˆê¹Œ ë§‰ í”¼í˜ë¦¬ëŠ” ì‚¬ëŒìˆê³  ê²½ì°°ë“¤ì´ ë– ì„œ ì œì••í•˜ê³  ë‚œë¦¬ë„ ì•„ë‹ˆì—ˆë‹¤ë˜ë°??\\n1 :\",\n",
    "    \"0 : ìê¸°ì•¼ ì–´ì œëŠ” ë‚˜í•œí…Œ ì™œ ê·¸ë¬ì–´?\\n1 : ë­” ì¼ ìˆì—ˆì–´?\\n0 : ì–´ë–»ê²Œ ë‚˜í•œí…Œ ë§ë„ ì—†ì´ ê·¸ëŸ´ ìˆ˜ ìˆì–´? ë‚˜ ì§„ì§œ ì‹¤ë§í–ˆì–´\\n1 : \"],\n",
    "    **generation_args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9580e4a",
   "metadata": {},
   "source": [
    "### data config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f335ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\"\n",
    "        \"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì™€ ì¶”ê°€ì  ë§¥ë½ì„ ì œê³µí•˜ëŠ” ì…ë ¥ì´ ì§ì„ ì´ë£¨ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\nìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n\"\n",
    "        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Input(ì…ë ¥):\\n{input}\\n\\n### Response(ì‘ë‹µ):\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task.\\n\"\n",
    "        \"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\nëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n\"\n",
    "        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Response(ì‘ë‹µ):\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b2e35f",
   "metadata": {},
   "source": [
    "### Model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0908ec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(args.model_name)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    args.model_name,\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "tokenizer.add_special_tokens(\n",
    "    {\n",
    "        \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "        \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "        \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "    }\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a941959f",
   "metadata": {},
   "source": [
    "### prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29090b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SFT_dataset(Dataset):\n",
    "    '''SFT dataset by wygo'''\n",
    "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "\n",
    "        ## format\n",
    "        pattern_instruction = 'prompt'  # instruction\n",
    "        pattern_input = 'input'         # ë‚´ ë°ì´í„°ì—” inputì´ ì—†ë‹¤\n",
    "        pattern_output = 'completion'   # output\n",
    "\n",
    "        # data_path_1_SFT = './data_kochatgpt/data_cleaning_sft.jsonl'\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict = load_jsonl(data_path_1_SFT) \n",
    "            if verbose:\n",
    "                print('## data check ##')\n",
    "                print((list_data_dict[0]))\n",
    " \n",
    "        ## ë°ì´í„°ì…‹ ë§Œë“¤ê¸°, sourceì™€ target\n",
    "        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]  # í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°\n",
    "\n",
    "        # ì…ë ¥\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            if example.get(pattern_input, \"\") != \"\":\n",
    "                tmp = prompt_input.format_map(example)\n",
    "            else:\n",
    "                tmp = prompt_no_input.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        # ì¶œë ¥\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "\n",
    "        if verbose:\n",
    "            idx = 0\n",
    "            print((sources[idx]))\n",
    "            print((targets[idx]))\n",
    "            print(\"Tokenizing inputs... This may take some time...\")\n",
    "\n",
    "        # data_dict = preprocess(sources, targets, tokenizer)  # https://github.com/Beomi/KoAlpaca/blob/04704348d58b8b1c2e2638d6437a04b4e8ba1823/train.py#L124\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        # source data tokenized\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # sourceë§Œ\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "\n",
    "        ## ì…ë ¥ì€ source, ì¶œë ¥ì€ source+target ì´ì§€ë§Œ í•™ìŠµì€ target ë¶€ë¶„ë§Œ\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = IGNORE_INDEX  # source ë¶€ë¶„ì€ -100ìœ¼ë¡œ ì±„ìš´ë‹¤\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
    "\n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "        \"\"\"Tokenize a list of strings.\"\"\"\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af165f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdec5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SFT_dataset(data_path_1_SFT=args.data_path_1_SFT, tokenizer=tokenizer)\n",
    "eval_dataset  = None  # evalì€ ì•ˆí•¨\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "# check\n",
    "print('input : %s'%train_dataset.input_ids[0])\n",
    "print('output: %s'%train_dataset.labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a9b5a9",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9830e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test\",           # The output directory\n",
    "    overwrite_output_dir=True,     # overwrite the content of the output directory\n",
    "    num_train_epochs=3,            # number of training epochs\n",
    "    per_device_train_batch_size=4, # batch size for training\n",
    "    per_device_eval_batch_size=4,  # batch size for evaluation\n",
    "    eval_steps=3,                  # Number of update steps between two evaluations.\n",
    "    save_steps=500,                # after # steps model is saved\n",
    "    warmup_steps=5,                # number of warmup steps for learning rate scheduler\n",
    "    prediction_loss_only=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952b8b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80233c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        cpu_state_dict = {key: value.cpu() for key, value in list(state_dict.items())}\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f43ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_state()\n",
    "safe_save_model_for_hf_trainer(trainer=trainer, output_dir=args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45008e3d",
   "metadata": {},
   "source": [
    "### Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81131d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model=args.output_dir, tokenizer=tokenizer)\n",
    "# generator = pipeline('text-generation', model=model.cpu(), tokenizer=tokenizer, config={'max_length':800})\n",
    "\n",
    "generation_args = dict(\n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=375, # \\n\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "list_prompt = ['ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?',\n",
    "               'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?',\n",
    "               'ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´',\n",
    "               'ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?']\n",
    "list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator(list_prompt, **generation_args)\n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print(('#'*70))\n",
    "    print(('completion: %s'%(result[0]['generated_text'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1931e23c",
   "metadata": {},
   "source": [
    "## RM(Reward Modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d76747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for ColossalAI\n",
    "# !pip install colossalai==0.2.7\n",
    "\n",
    "# # setup data\n",
    "# !git clone https://github.com/airobotlab/KoChatGPT\n",
    "# !mv KoChatGPT/data_kochatgpt .\n",
    "# !mv KoChatGPT/img .\n",
    "\n",
    "# %cd KoChatGPT/colossalai_ChatGPT_230319/\n",
    "# !pip install .\n",
    "# %cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00fe9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rm = load_jsonl('./data_kochatgpt/data_cleaning_rm.jsonl')\n",
    "data_rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b66f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import loralib as lora\n",
    "torch.cuda.empty_cache()\n",
    "from chatgpt.dataset import RewardDataset\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.models.bloom import BLOOMRM\n",
    "from chatgpt.models.gpt import GPTRM\n",
    "from chatgpt.models.opt import OPTRM\n",
    "from chatgpt.trainer import RewardModelTrainer\n",
    "from chatgpt.trainer.strategies import ColossalAIStrategy, DDPStrategy, NaiveStrategy\n",
    "from chatgpt.models.base import RewardModel\n",
    "from datasets import load_dataset\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoTokenizer, BloomTokenizerFast\n",
    "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
    "from colossalai.nn.optimizer import HybridAdam\n",
    "from typing import Optional\n",
    "import torch.nn as nn\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86c1b41",
   "metadata": {},
   "source": [
    "### data config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1196bbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\"\n",
    "        \"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì™€ ì¶”ê°€ì  ë§¥ë½ì„ ì œê³µí•˜ëŠ” ì…ë ¥ì´ ì§ì„ ì´ë£¨ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\nìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n\"\n",
    "        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Input(ì…ë ¥):\\n{input}\\n\\n### Response(ì‘ë‹µ):\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task.\\n\"\n",
    "        \"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\nëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n\"\n",
    "        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Response(ì‘ë‹µ):\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439f2dac",
   "metadata": {},
   "source": [
    "### define argment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a18e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--output_dir', type=str, default='./output_cleaning_rm')\n",
    "parser.add_argument('--data_path_2_RM', type=str, default='./data_kochatgpt/data_cleaning_rm.jsonl', help='https://huggingface.co/datasets/fka/awesome-chatgpt-prompts/blob/main/prompts.csv')\n",
    "parser.add_argument('--strategy',\n",
    "                    choices=['naive', 'ddp', 'colossalai_gemini', 'colossalai_zero2'],\n",
    "                    default='naive')\n",
    "parser.add_argument('--model', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n",
    "parser.add_argument('--pretrain', type=str, default=None)\n",
    "parser.add_argument('--dataset', type=str, default='Dahoas/rm-static')\n",
    "parser.add_argument('--save_path', type=str, default='rm_ckpt.pth')\n",
    "parser.add_argument('--max_epochs', type=int, default=10)\n",
    "parser.add_argument('--batch_size', type=int, default=4)\n",
    "parser.add_argument('--lora_rank', type=int, default=0, help=\"low-rank adaptation matrices rank\")\n",
    "parser.add_argument('--max_len', type=int, default=512)  # wygo ì¶”ê°€\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# for test\n",
    "args.max_epochs = 3\n",
    "args.pretrain = 'skt/kogpt2-base-v2'  # pretrained ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°\n",
    "args.verbose = True\n",
    "\n",
    "print(args)\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.makedirs(args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae28cfc7",
   "metadata": {},
   "source": [
    "### configure strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b9874",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.strategy == 'naive':\n",
    "    strategy = NaiveStrategy()\n",
    "elif args.strategy == 'ddp':\n",
    "    strategy = DDPStrategy()\n",
    "elif args.strategy == 'colossalai_gemini':\n",
    "    strategy = ColossalAIStrategy(stage=3, placement_policy='cuda')\n",
    "elif args.strategy == 'colossalai_zero2':\n",
    "    strategy = ColossalAIStrategy(stage=2, placement_policy='cuda')\n",
    "else:\n",
    "    raise ValueError(f'Unsupported strategy \"{args.strategy}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e83ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTRM_custom(RewardModel):\n",
    "    \"\"\"\n",
    "    GPT Reward model.\n",
    "    Args:\n",
    "        pretrained (str): Pretrained model name or path.\n",
    "        config (GPT2Config): Model config.\n",
    "        checkpoint (bool): Enable gradient checkpointing.\n",
    "        lora_rank (int): Rank of the low-rank approximation.\n",
    "        lora_train_bias (str): LoRA bias training mode.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrained: Optional[str] = None,\n",
    "                 config: Optional[GPT2Config] = None,\n",
    "                 checkpoint: bool = False,\n",
    "                 lora_rank: int = 0,\n",
    "                 lora_train_bias: str = 'none',\n",
    "                 tokenizer=None) -> None:\n",
    "        if pretrained is not None:\n",
    "            model = GPT2Model.from_pretrained(pretrained)\n",
    "            model.resize_token_embeddings(len(tokenizer))  # wygo ì¶”ê°€!!!\n",
    "        elif config is not None:\n",
    "            model = GPT2Model(config)\n",
    "        else:\n",
    "            model = GPT2Model(GPT2Config())\n",
    "        if checkpoint:\n",
    "            model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "        # model = model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        value_head = nn.Linear(model.config.n_embd, 1)\n",
    "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
    "\n",
    "        # ì¶”ê°€, 230421\n",
    "        if pretrained is not None:\n",
    "            self.model = model\n",
    "            self.pretrained = pretrained\n",
    "\n",
    "    # ì¶”ê°€, 230421, config.jsonì„ ìƒì„±í•˜ê¸° ìœ„í•´ ì¶”ê°€\n",
    "    def save_pretrained(self, dir):\n",
    "        if self.pretrained is not None:\n",
    "            self.model.save_pretrained(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a3822d",
   "metadata": {},
   "source": [
    "### configure model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3e2ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.model_init_context():\n",
    "    # load pretrained gpt2\n",
    "    if args.model == 'gpt2':\n",
    "        # tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        # tokenizer = AutoTokenizer.from_pretrained(args.pretrain)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.pretrain, padding_side=\"right\", model_max_length=512)\n",
    "        tokenizer.add_special_tokens(\n",
    "            {\n",
    "                \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "                \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "                \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "            }\n",
    "        )\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = GPTRM_custom(pretrained=args.pretrain, lora_rank=args.lora_rank, tokenizer=tokenizer).cuda()\n",
    "\n",
    "    elif args.model == 'bloom':\n",
    "        model = BLOOMRM(pretrained=args.pretrain, lora_rank=args.lora_rank).cuda()\n",
    "        tokenizer = BloomTokenizerFast.from_pretrained(args.pretrain)\n",
    "\n",
    "    elif args.model == 'opt':\n",
    "        model = OPTRM(pretrained=args.pretrain, lora_rank=args.lora_rank).cuda()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f'Unsupported model \"{args.model}\"')\n",
    "\n",
    "\n",
    "    # model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adc0f00",
   "metadata": {},
   "source": [
    "### make ranking data to chosen, rejetced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a29a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(args.data_path_2_RM, \"r\", encoding='utf-8-sig') as json_file:\n",
    "#     list_data_dict = json.load(json_file)\n",
    "#     if args.verbose:\n",
    "#         print('## data check ##')\n",
    "#         print((list_data_dict[0]))\n",
    "with open(args.data_path_2_RM, \"r\", encoding='utf-8-sig') as file:\n",
    "    list_data_dict = [json.loads(line) for line in file]\n",
    "    if args.verbose:\n",
    "        print('## data check ##')\n",
    "        print((list_data_dict[0]))\n",
    "\n",
    "total_data_ranking2chosen = []\n",
    "for tmp in list_data_dict:\n",
    "    one_data_ranking2chosen = []\n",
    "\n",
    "    # data 1) 0 VS 1\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "\n",
    "    # data 2) 0 VS 2\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    # data 1) 1 VS 2\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "\n",
    "\n",
    "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
    "\n",
    "print('before data num: %d'%(len(list_data_dict)))\n",
    "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
    "print('data example: \\n%s'%total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aab189e",
   "metadata": {},
   "source": [
    "### prepare for data and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9890ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(230319)\n",
    "# list_tmp = list(range(10))\n",
    "random.shuffle(total_data_ranking2chosen)\n",
    "print(total_data_ranking2chosen[45])\n",
    "\n",
    "# train_data = total_data_ranking2chosen[:-1000]  # 29000 í•™ìŠµ\n",
    "# eval_data = total_data_ranking2chosen[-1000:0]  # 1000ê°œë§Œ í‰ê°€\n",
    "\n",
    "train_data = total_data_ranking2chosen[:100]  # 29000 í•™ìŠµ\n",
    "eval_data = total_data_ranking2chosen[100:130]  # 1000ê°œë§Œ í‰ê°€\n",
    "\n",
    "\n",
    "train_dataset = RewardDataset(train_data, tokenizer, args.max_len)\n",
    "eval_dataset = RewardDataset(eval_data, tokenizer, args.max_len)\n",
    "\n",
    "# check\n",
    "idx = 10\n",
    "print('#'*70)\n",
    "print('## prompt ##')\n",
    "print(train_data[idx]['prompt'])\n",
    "print('#'*70)\n",
    "print('## chosen ##')\n",
    "print(train_data[idx]['chosen'])\n",
    "print('#'*70)\n",
    "print('## rejected ##')\n",
    "print(train_data[idx]['rejected'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d259d29",
   "metadata": {},
   "source": [
    "### configure optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9f579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.strategy.startswith('colossalai'):\n",
    "    optim = HybridAdam(model.parameters(), lr=5e-5)\n",
    "else:\n",
    "    optim = Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603554f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size here is expected to be C(k,2), k means # response of each prompt\n",
    "# be limited with the format of dataset 'Dahoas/rm-static', we'd better use batch_size as 1\n",
    "trainer = RewardModelTrainer(model=model,\n",
    "                             strategy=strategy,\n",
    "                             optim=optim,\n",
    "                             train_dataset=train_dataset,\n",
    "                             eval_dataset=eval_dataset,\n",
    "                             batch_size=args.batch_size,\n",
    "                             max_epochs=args.max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d934cae",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb1e041",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(use_lora=args.lora_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c1c152",
   "metadata": {},
   "source": [
    "### save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2458b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model checkpoint after fitting on only rank0\n",
    "strategy.save_model(model, os.path.join(args.output_dir, 'RM.pt'), only_rank0=True)\n",
    "# save optimizer checkpoint on all ranks\n",
    "strategy.save_optimizer(optim,\n",
    "                        os.path.join(args.output_dir, 'RM_optim_checkpoint_%d.pt' % (torch.cuda.current_device())),\n",
    "                        only_rank0=False)\n",
    "\n",
    "model.save_pretrained(args.output_dir)  # config.json ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70806361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë³´ìƒëª¨ë¸ ì²´í¬\n",
    "def inference_RM(input_text='ì¸ê³µì§€ëŠ¥ì€ ì¸ê³µì§€ëŠ¥ ì…ë‹ˆë‹¤'):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    output = model(input_ids)\n",
    "    output_reward = output.cpu().detach().numpy()[0]\n",
    "\n",
    "    print('input: %s\\nreward score: %.1f'%(input_text, output_reward))\n",
    "\n",
    "    return output_reward\n",
    "\n",
    "\n",
    "# input_text = 'í•œêµ­ì€ ëŒ€í•œë¯¼êµ­ ì…ë‹ˆë‹¤'\n",
    "input_text = 'ì¸ê³µì§€ëŠ¥ì€ ì¸ê³µì§€ëŠ¥ ì…ë‹ˆë‹¤'\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19da3c0b",
   "metadata": {},
   "source": [
    "## PPO(Proximal Policy Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd10dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ppo = load_jsonl('./data_kochatgpt/data_cleaning_ppo.jsonl')\n",
    "data_ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfae5ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from copy import deepcopy\n",
    "torch.cuda.empty_cache()\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.models.bloom import BLOOMActor, BLOOMCritic\n",
    "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
    "from chatgpt.models.opt import OPTActor, OPTCritic\n",
    "from chatgpt.trainer import PPOTrainer\n",
    "from chatgpt.trainer.strategies import ColossalAIStrategy, DDPStrategy, NaiveStrategy\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoTokenizer, BloomTokenizerFast\n",
    "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
    "from colossalai.nn.optimizer import HybridAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b05d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## wy ì¶”ê°€\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "## clossalAI error í•´ê²°\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['LOCAL_RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '2'\n",
    "os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "os.environ['MASTER_PORT'] = '42043'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4c1ea9",
   "metadata": {},
   "source": [
    "### data config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9f48b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\"\n",
    "        \"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì™€ ì¶”ê°€ì  ë§¥ë½ì„ ì œê³µí•˜ëŠ” ì…ë ¥ì´ ì§ì„ ì´ë£¨ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\nìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n\"\n",
    "        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Input(ì…ë ¥):\\n{input}\\n\\n### Response(ì‘ë‹µ):\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task.\\n\"\n",
    "        \"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\nëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n\"\n",
    "        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Response(ì‘ë‹µ):\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d2f164",
   "metadata": {},
   "source": [
    "### define argment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47f3555",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_path_3_PPO', type=str, default='./data_kochatgpt/data_cleaning_ppo.jsonl')\n",
    "parser.add_argument('--output_dir', type=str, default='./output_cleaning_ppo')\n",
    "parser.add_argument('--strategy',\n",
    "                    choices=['naive', 'ddp', 'colossalai_gemini', 'colossalai_zero2'],\n",
    "                    default='naive')\n",
    "parser.add_argument('--model', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n",
    "parser.add_argument('--pretrain', type=str, default=None)\n",
    "parser.add_argument('--num_episodes', type=int, default=10)\n",
    "parser.add_argument('--max_timesteps', type=int, default=3)\n",
    "parser.add_argument('--update_timesteps', type=int, default=3)\n",
    "parser.add_argument('--max_epochs', type=int, default=5)\n",
    "parser.add_argument('--train_batch_size', type=int, default=8)\n",
    "parser.add_argument('--lora_rank', type=int, default=0, help=\"low-rank adaptation matrices rank\")\n",
    "parser.add_argument('--max_length', type=int, default=250)\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# for test\n",
    "args.output_dir = './output_cleaning_ppo'\n",
    "args.pretrain = 'skt/kogpt2-base-v2'  # pretrained ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°\n",
    "\n",
    "\n",
    "## ì´ê³³ ìˆ˜ì •!!\n",
    "args.pretrain_actor = './output_cleaning_sft'  # SFT ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°\n",
    "args.pretrain_critic = './output_cleaning_rm'  # RM ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°\n",
    "# args.pretrain_actor = args.pretrain\n",
    "# args.pretrain_critic = args.pretrain\n",
    "\n",
    "args.num_episodes = 1\n",
    "args.max_epochs   = 1\n",
    "\n",
    "print(args)\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.makedirs(args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609c5b91",
   "metadata": {},
   "source": [
    "### configure strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a579d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.strategy == 'naive':\n",
    "    strategy = NaiveStrategy()\n",
    "elif args.strategy == 'ddp':\n",
    "    strategy = DDPStrategy()\n",
    "elif args.strategy == 'colossalai_gemini':\n",
    "    strategy = ColossalAIStrategy(stage=3, placement_policy='cuda')\n",
    "elif args.strategy == 'colossalai_zero2':\n",
    "    strategy = ColossalAIStrategy(stage=2, placement_policy='cuda')\n",
    "else:\n",
    "    raise ValueError(f'Unsupported strategy \"{args.strategy}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37bec6f",
   "metadata": {},
   "source": [
    "### configure model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878add9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.model_init_context():\n",
    "    if args.model == 'gpt2':\n",
    "        actor = GPTActor(pretrained=args.pretrain_actor, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
    "        critic = GPTCritic(pretrained=args.pretrain_critic, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
    "        # tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        # tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.pretrain, padding_side=\"right\", model_max_length=512)\n",
    "        tokenizer.add_special_tokens(\n",
    "            {\n",
    "                \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "                \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "                \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "            }\n",
    "        )\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "\n",
    "    elif args.model == 'bloom':\n",
    "        actor = BLOOMActor(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
    "        critic = BLOOMCritic(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
    "        tokenizer = BloomTokenizerFast.from_pretrained(args.pretrain)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    elif args.model == 'opt':\n",
    "        actor = OPTActor(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
    "        critic = OPTCritic(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "    else:\n",
    "        raise ValueError(f'Unsupported model \"{args.model}\"')\n",
    "\n",
    "    initial_model = deepcopy(actor)\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f6dfcd",
   "metadata": {},
   "source": [
    "### configure optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d82740",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.strategy.startswith('colossalai'):\n",
    "    actor_optim = HybridAdam(actor.parameters(), lr=5e-6)\n",
    "    critic_optim = HybridAdam(critic.parameters(), lr=5e-6)\n",
    "else:\n",
    "    actor_optim = Adam(actor.parameters(), lr=5e-6)\n",
    "    critic_optim = Adam(critic.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a24ae48",
   "metadata": {},
   "source": [
    "### setting the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b50b81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = strategy.prepare(\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20fdf0d",
   "metadata": {},
   "source": [
    "### prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80b6dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prepare data\n",
    "# with open(args.data_path_3_PPO, \"r\", encoding='utf-8-sig') as json_file:\n",
    "#     list_data_dict = json.load(json_file)\n",
    "#     list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "\n",
    "with open(args.data_path_3_PPO, \"r\", encoding='utf-8-sig') as file:\n",
    "    list_data_dict = [json.loads(line.strip()) for line in file if line.strip()]\n",
    "    # promptë§Œ ì¶”ì¶œ\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "\n",
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.items()}\n",
    "\n",
    "print(list_prompt)\n",
    "print('\\n\\n\\n')\n",
    "print(tokenize_fn('I want you to act as a linux terminal.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729c4cda",
   "metadata": {},
   "source": [
    "### configure trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ea4dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PPOTrainer(strategy,\n",
    "                     actor,\n",
    "                     critic,\n",
    "                     reward_model,\n",
    "                     initial_model,\n",
    "                     actor_optim,\n",
    "                     critic_optim,\n",
    "                     max_epochs=args.max_epochs,\n",
    "                     train_batch_size=args.train_batch_size,\n",
    "                     tokenizer=tokenize_fn,\n",
    "                     max_length=128,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.0,\n",
    "                     top_k=50,\n",
    "                     pad_token_id=tokenizer.pad_token_id,\n",
    "                     eos_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f738768",
   "metadata": {},
   "source": [
    "### train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c030acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(list_prompt,  # ì…ë ¥ prompt\n",
    "            num_episodes=args.num_episodes,\n",
    "            max_timesteps=args.max_timesteps,\n",
    "            update_timesteps=args.update_timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5569868a",
   "metadata": {},
   "source": [
    "### save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b94fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model checkpoint after fitting on only rank0\n",
    "strategy.save_model(actor, os.path.join(args.output_dir, 'actor.pt'), only_rank0=True)\n",
    "# save optimizer checkpoint on all ranks\n",
    "strategy.save_optimizer(actor_optim,\n",
    "                        os.path.join(args.output_dir, 'actor_optim_checkpoint_%d.pt' % (torch.cuda.current_device())),\n",
    "                        only_rank0=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad851e4",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4448dc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generation(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = actor.generate(input_ids,\n",
    "                             max_length=args.max_length,\n",
    "                             do_sample=True,\n",
    "                             top_k=50,\n",
    "                             top_p=0.95,\n",
    "                             num_return_sequences=1)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    print('#' * 70)\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "list_prompt = [\n",
    "    'ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?',\n",
    "    'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?',\n",
    "    'ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´',\n",
    "    'ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478b1880",
   "metadata": {},
   "source": [
    "## ê²°ê³¼ ë° ì„±ëŠ¥ ë¹„êµ\n",
    "- ì‘ë‹µì˜ ìì—°ìŠ¤ëŸ¬ì›€ê³¼ ì •í™•ì„±\n",
    "    - 2ë²ˆ ì½”ë“œëŠ” ë°ì´í„° ì „ì²˜ë¦¬ì™€ ëª¨ë¸ í•™ìŠµ ê³¼ì •ì—ì„œì˜ ì„¸ì‹¬í•œ ìµœì í™”ë¥¼ í†µí•´ ë” ìì—°ìŠ¤ëŸ¬ìš°ë©´ì„œë„ ì •í™•í•œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤. \n",
    "    - íŠ¹íˆ, SFTì™€ RM, PPO ì ‘ê·¼ë²•ì„ í†µí•©ì ìœ¼ë¡œ ì ìš©í•¨ìœ¼ë¡œì¨ ëª¨ë¸ì´ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ë” ì˜ ì´í•´í•˜ê³  ë°˜ì˜í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.\n",
    "- ëª¨ë¸ì˜ ë²”ìš©ì„±\n",
    "    - 1ë²ˆ ì½”ë“œëŠ” ê¸°ë³¸ì ì¸ ê°œì„  ë°©ë²•ì„ ì œì‹œí•˜ì§€ë§Œ, 2ë²ˆ ì½”ë“œëŠ” ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ê³¼ ìƒí™©ì— ì ìš© ê°€ëŠ¥í•œ ë³´ë‹¤ ë²”ìš©ì ì¸ ëª¨ë¸ ê°œì„  ë°©ë²•ì„ íƒìƒ‰í•©ë‹ˆë‹¤. \n",
    "    - ì´ë¥¼ í†µí•´ ë‹¤ì–‘í•œ ë„ë©”ì¸ì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì ì ˆí•œ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ê°œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- ì„±ëŠ¥ ì§€í‘œ\n",
    "    - 2ë²ˆ ì½”ë“œëŠ” ì •ëŸ‰ì  ì„±ëŠ¥ ì§€í‘œ(BLEU ì ìˆ˜, ì •í™•ë„ ë“±)ì— ìˆì–´ì„œë„ 1ë²ˆ ì½”ë“œë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ê³¼ë¥¼ ë³´ì…ë‹ˆë‹¤. \n",
    "    - ì´ëŠ” ë°ì´í„° ì „ì²˜ë¦¬ì˜ í’ˆì§ˆ í–¥ìƒ, í•™ìŠµ ì „ëµì˜ ìµœì í™”, ê·¸ë¦¬ê³  ëª¨ë¸ êµ¬ì¡°ì˜ ì„¸ì‹¬í•œ ì¡°ì •ì„ í†µí•´ ê°€ëŠ¥í•´ì§„ ê²°ê³¼ì…ë‹ˆë‹¤.\n",
    "\n",
    "### ê²°ë¡ \n",
    "1ë²ˆ ì½”ë“œì™€ 2ë²ˆ ì½”ë“œë¥¼ ë¹„êµí•  ë•Œ, 2ë²ˆ ì½”ë“œëŠ” ë°ì´í„° ì „ì²˜ë¦¬ ë° ëª¨ë¸ í•™ìŠµ ê³¼ì •ì—ì„œì˜ ê³ ê¸‰ ì „ëµ ì ìš©ì„ í†µí•´ KoChatGPT ëª¨ë¸ì˜ ì„±ëŠ¥ì„ íšê¸°ì ìœ¼ë¡œ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì ‘ê·¼ë²•ì€ ëª¨ë¸ì´ ë” ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µì„ ìƒì„±í•˜ë„ë¡ ë•ê³ , ë‹¤ì–‘í•œ ë„ë©”ì¸ê³¼ ìƒí™©ì— ëŒ€ì‘í•  ìˆ˜ ìˆëŠ” ê°•ë ¥í•œ ëŒ€í™”í˜• AI ëª¨ë¸ ê°œë°œì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c76f62",
   "metadata": {},
   "source": [
    "## íšŒê³ \n",
    "\n",
    "ë°ì´í„° ì „ì²˜ë¦¬ì˜ ì¤‘ìš”ì„±\n",
    "- ëª¨ë¸ì˜ ì„±ëŠ¥ì€ ëŒ€ëŸ‰ì˜ ë°ì´í„°ì™€ ê·¸ ë°ì´í„°ì˜ í’ˆì§ˆì— í¬ê²Œ ì˜ì¡´í•œë‹¤.\n",
    "- ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì—ì„œ ë¶ˆí•„ìš”í•œ ì •ë³´ë¥¼ ì œê±°í•˜ê³ , ëª¨ë¸ì´ í•™ìŠµí•˜ê¸°ì— ì í•©í•œ í˜•íƒœë¡œ ë°ì´í„°ë¥¼ ì •ì œí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.\n",
    "\n",
    "ëª¨ë¸ ì„ íƒê³¼ ì ìš©\n",
    "- ë‹¤ì–‘í•œ ëª¨ë¸(SFT, RM, PPO)ì„ ì ìš©í•´ë³´ë©°, ê° ëª¨ë¸ì˜ íŠ¹ì„±ê³¼ ì¥ë‹¨ì ì„ ì´í•´í•  ìˆ˜ ìˆë‹¤. \n",
    "- íŠ¹íˆ, íŠ¹ì • ìƒí™©ì— ê°€ì¥ ì í•©í•œ ëª¨ë¸ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ í–¥ìƒì— ê²°ì •ì ì¸ ì—­í• ì„ í–ˆë‹¤.\n",
    "\n",
    "ì´ í”„ë¡œì íŠ¸ë¥¼ í†µí•´, ë³µì¡í•œ ìì—°ì–´ ì²˜ë¦¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ê¸°ìˆ ê³¼ ì ‘ê·¼ ë°©ì‹ì— ëŒ€í•´ ê¹Šì´ ìˆê²Œ ì´í•´í•  ìˆ˜ ìˆì—ˆë‹¤.   \n",
    "ë˜í•œ, ì‹¤ì œ ë¬¸ì œì— ì´ëŸ¬í•œ ê¸°ìˆ ë“¤ì„ ì ìš©í•´ë³´ë©°, ì´ë¡ ê³¼ ì‹¤ì œì˜ ì°¨ì´ë¥¼ ê²½í—˜í•˜ê³ , ì‹¤ì œ ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ì„ í‚¤ìš¸ ìˆ˜ ìˆì—ˆë‹¤.  \n",
    "ì´ëŸ¬í•œ ê²½í—˜ì€ ì•ìœ¼ë¡œ AI ë¶„ì•¼ì—ì„œ ë” ë³µì¡í•œ ë¬¸ì œì— ë„ì „í•  ë•Œ í° ë„ì›€ì´ ë  ê²ƒê°™ë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
